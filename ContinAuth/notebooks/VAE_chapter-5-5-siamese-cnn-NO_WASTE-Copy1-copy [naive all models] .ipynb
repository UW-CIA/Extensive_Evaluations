{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese CNN & vae\n",
    "\n",
    "*Created by Holger Buech, Q1/2019*\n",
    "\n",
    "**Description**   \n",
    "\n",
    "Reimplemenation of an approach to Continuous Authentication described by [1]. It leverages a Siamese CNN to generate Deep Features, which are then used as input for an OCSVM authentication classifier.  \n",
    "\n",
    "**Purpose**\n",
    "\n",
    "- Verify results of [1]\n",
    "- Test the approach with upfront global subject wise normalization (NAIVE_APPROACH)\n",
    "- Change the normalization setting to be more realistic: Training data is normalized upfront again, but the Testing data is normalized using a single scaler fitted on training data only. (VALID_APPROACH)\n",
    "- Identify parameters performing better in a valid setup than the parameters proposed by [1]. (ALTERNATIVE_APPROACH) \n",
    "\n",
    "**Data Sources**   \n",
    "\n",
    "- [H-MOG Dataset](http://www.cs.wm.edu/~qyang/hmog.html)  \n",
    "  (Downloaded beforehand using  [./src/data/make_dataset.py](./src/data/make_dataset.py), stored in [./data/external/hmog_dataset/](./data/external/hmog_dataset/) and converted to [./data/processed/hmog_dataset.hdf5](./data/processed/hmog_dataset.hdf5))\n",
    "\n",
    "**References**   \n",
    "\n",
    "- [1] Centeno, M. P. et al. (2018): Mobile Based Continuous Authentication Using Deep Features. Proceedings of the 2^nd International Workshop on Embedded and Mobile Deep Learning (EMDL), 2018, 19-24.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "**1 - [Preparations](#1)**   \n",
    "1.1 - [Imports](#1.1)   \n",
    "1.2 - [Configuration](#1.2)   \n",
    "1.3 - [Experiment Parameters](#1.3)   \n",
    "1.4 - [Select Approach](#1.4)   \n",
    "\n",
    "**2 - [Initial Data Prepratation](#2)**   \n",
    "2.1 - [Load Dataset](#2.1)   \n",
    "2.2 - [Normalize Features (if global)](#2.2)   \n",
    "2.3 - [Split Dataset for Valid/Test](#2.3)   \n",
    "2.4 - [Normalize Features (if not global)](#2.4)   \n",
    "2.5 - [Check Splits](#2.5)   \n",
    "2.6 - [Reshape Features](#2.6)     \n",
    "\n",
    "**3 - [Generate Scenario Pairs](#3)**    \n",
    "3.1 - [Load cached Data](#3.1)  \n",
    "3.2 - [Build positive/negative Pairs](#3.2)  \n",
    "3.3 - [Inspect Pairs](#3.3)  \n",
    "3.4 - [Cache Pairs](#3.4)  \n",
    "\n",
    "**4 - [Siamese Network](#4)**  \n",
    "4.1 - [Load cached Pairs](#4.1)   \n",
    "4.2 - [Build Model](#4.2)   \n",
    "4.3 - [Prepare Features](#4.3)   \n",
    "4.4 - [Search optimal Epoch](#4.4)   \n",
    "4.5 - [Check Distances](#4.5)   \n",
    "4.6 - [Rebuild and train to optimal Epoch](#4.6)   \n",
    "4.7 - [Cache Model](#4.7)   \n",
    "\n",
    "**5 - [Visualize Deep Features](#5)**   \n",
    "5.1 - [Load cached Data](#5.1)  \n",
    "5.2 - [Extract CNN from Siamese Model](#5.2)  \n",
    "5.3 - [Test Generation of Deep Features](#5.3)  \n",
    "5.4 - [Visualize in 2D using PCA](#5.4)  \n",
    "\n",
    "**6 - [OCSVM](#6)**  \n",
    "6.1 - [Load cached Data](#6.1)  \n",
    "6.2 - [Load trained Siamese Model](#6.2)  \n",
    "6.3 - [Search for Parameters](#6.3)  \n",
    "6.4 - [Inspect Search Results](#6.4) \n",
    "\n",
    "**7 - [Testing](#7)**  \n",
    "7.1 - [Load cached Data](#7.1)  \n",
    "7.2 - [Evaluate Auth Performance](#7.2)  \n",
    "7.3 - [Evaluate increasing Training Set Size (Training Delay)](#7.3)  \n",
    "7.4 - [Evaluate increasing Test Set Sizes (Detection Delay)](#7.4)  \n",
    "\n",
    "**8 - [Report Results](#8)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 1. Preparations <a id='1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports <a id='1.1'>&nbsp;</a> \n",
    "**Note:** The custom `DatasetLoader` is a helper class for easier loading and subsetting data from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import random\n",
    "import dataclasses\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Extra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.stats.api as sms\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Lambda,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    Activation\n",
    ")\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD,RMSprop\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Custom\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))  # supposed to be parent folder\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.utility.dataset_loader_hdf5 import DatasetLoader\n",
    "\n",
    "# Global utitlity functions are loaded from separate notebook:\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration <a id='1.2'>&nbsp;</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Data Loading & Seed\n",
    "SEED = 712  # Used for every random function\n",
    "HMOG_HDF5 = Path.cwd().parent / \"data\" / \"processed\" / \"hmog_dataset.hdf5\"\n",
    "EXCLUDE_COLS = [\"sys_time\"]\n",
    "CORES = mp.cpu_count()\n",
    "\n",
    "# For plots and CSVs\n",
    "OUTPUT_PATH = Path.cwd() / \"output\" / \"chapter-6-1-4-siamese-cnn\"  # Cached data & csvs\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_PATH = Path.cwd().parent / \"reports\" / \"figures\" # Figures for thesis\n",
    "REPORT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Improve performance of Tensorflow (this improved speed _a_lot_ on my machine!!!)\n",
    "K.tf.set_random_seed(SEED)\n",
    "conf = K.tf.ConfigProto(\n",
    "    device_count={\"CPU\": CORES},\n",
    "    allow_soft_placement=True,\n",
    "    intra_op_parallelism_threads=CORES,\n",
    "    inter_op_parallelism_threads=CORES,\n",
    ")\n",
    "K.set_session(K.tf.Session(config=conf))\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "utils_set_output_style()\n",
    "\n",
    "# Silence various deprecation warnings...\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "np.warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround to remove ugly spacing between tqdm progress bars:\n",
    "HTML(\"<style>.p-Widget.jp-OutputPrompt.jp-OutputArea-prompt:empty{padding: 0;border: 0;} div.output_subarea{padding:0;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Experiment Parameters <a id='1.3'>&nbsp;</a> \n",
    "Selection of parameters set that had been tested in this notebook. Select one of them to reproduce results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ExperimentParameters:\n",
    "    \"\"\"Contains all relevant parameters to run an experiment.\"\"\"\n",
    "\n",
    "    name: str  # Name of Experiments Parameter set. Used as identifier for charts etc.\n",
    "\n",
    "    # Data / Splitting:\n",
    "    frequency: int\n",
    "    feature_cols: list  # Columns used as features\n",
    "    max_subjects: int\n",
    "    exclude_subjects: list  # Don't load data from those users\n",
    "    n_valid_train_subjects: int\n",
    "    n_valid_test_subjects: int\n",
    "    n_test_train_subjects: int\n",
    "    n_test_test_subjects: int\n",
    "    seconds_per_subject_train: float\n",
    "    seconds_per_subject_test: float\n",
    "    task_types: list  # Limit scenarios to [1, 3, 5] for sitting or [2, 4, 6] for walking, or don't limit (None)\n",
    "\n",
    "    # Reshaping\n",
    "    window_size: int  # After resampling\n",
    "    step_width: int  # After resampling\n",
    "\n",
    "    # Normalization\n",
    "    scaler: str  # {\"std\", \"robust\", \"minmax\"}\n",
    "    scaler_scope: str  # {\"subject\", \"session\"}\n",
    "    scaler_global: bool  # scale training and testing sets at once (True), or fit scaler on training only (False)\n",
    "\n",
    "    # Siamese Network\n",
    "    max_pairs_per_session: int  # Max. number of pairs per session\n",
    "    margin: float  # Contrastive Loss Margin\n",
    "    model_variant: str  # {\"1d\", \"2d\"} Type of architecture\n",
    "    filters: list  # List of length 4, containing number of filters for conv layers\n",
    "    epochs_best: int  # Train epochs to for final model\n",
    "    epochs_max: int\n",
    "    batch_size: int\n",
    "    optimizer: str  # Optimizer to use for Siamese Network\n",
    "    optimizer_lr: float  # Learning Rate\n",
    "    optimizer_decay: float\n",
    "\n",
    "    # OCSVM\n",
    "    ocsvm_nu: float  # Best value found in random search, used for final model\n",
    "    ocsvm_gamma: float  # Best value found in random search, used for final model\n",
    "\n",
    "    # Calculated values\n",
    "    def __post_init__(self):\n",
    "        # HDF key of table:\n",
    "        self.table_name = f\"sensors_{self.frequency}hz\"\n",
    "\n",
    "        # Number of samples per _session_ used for training:\n",
    "        self.samples_per_subject_train = math.ceil(\n",
    "            (self.seconds_per_subject_train * 100)\n",
    "            / (100 / self.frequency)\n",
    "            / self.window_size\n",
    "        )\n",
    "\n",
    "        # Number of samples per _session_ used for testing:\n",
    "        self.samples_per_subject_test = math.ceil(\n",
    "            (self.seconds_per_subject_test * 100)\n",
    "            / (100 / self.frequency)\n",
    "            / self.window_size\n",
    "        )\n",
    "\n",
    "\n",
    "# INSTANCES\n",
    "# ===========================================================\n",
    "\n",
    "# NAIVE_MINMAX (2D Filters)\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_MINMAX_2D = ExperimentParameters(\n",
    "    name=\"NAIVE-MINMAX-2D\",\n",
    "    # Data / Splitting\n",
    "    frequency=25,\n",
    "    feature_cols=[\n",
    "        \"acc_x\",\n",
    "        \"acc_y\",\n",
    "        \"acc_z\",\n",
    "        \"gyr_x\",\n",
    "        \"gyr_y\",\n",
    "        \"gyr_z\",\n",
    "        \"mag_x\",\n",
    "        \"mag_y\",\n",
    "        \"mag_z\",\n",
    "    ],\n",
    "    max_subjects=90,\n",
    "    exclude_subjects=[\n",
    "        \"733162\",  # No 24 sessions\n",
    "        \"526319\",  # ^\n",
    "        \"796581\",  # ^\n",
    "        \"539502\",  # Least amount of sensor values\n",
    "        \"219303\",  # ^\n",
    "        \"737973\",  # ^\n",
    "        \"986737\",  # ^\n",
    "        \"256487\",  # Most amount of sensor values\n",
    "        \"389015\",  # ^\n",
    "        \"856401\",  # ^\n",
    "    ],\n",
    "    n_valid_train_subjects=40,\n",
    "    n_valid_test_subjects=10,\n",
    "    n_test_train_subjects=10,\n",
    "    n_test_test_subjects=30,\n",
    "    seconds_per_subject_train=67.5,\n",
    "    seconds_per_subject_test=67.5,\n",
    "    task_types=None,\n",
    "    # Reshaping\n",
    "    window_size=25,  # 1 sec\n",
    "    step_width=25,\n",
    "    # Normalization\n",
    "    scaler=\"minmax\",\n",
    "    scaler_scope=\"subject\",\n",
    "    scaler_global=True,\n",
    "    # Siamese Network\n",
    "    model_variant=\"2d\",\n",
    "    filters=[32, 64, 128, 32],\n",
    "    epochs_best=35,\n",
    "    epochs_max=40,\n",
    "    batch_size=200,\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.01,\n",
    "    optimizer_decay=0,\n",
    "    max_pairs_per_session=60,  # => 4min\n",
    "    margin=0.2,\n",
    "    # OCSVM\n",
    "    ocsvm_nu=0.092,\n",
    "    ocsvm_gamma=1.151,\n",
    ")  # <END NAIVE_APPROACH>\n",
    "\n",
    "# VALID_MINMAX (2D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_MINMAX_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-MINMAX-2D\",\n",
    "    task_types=None,\n",
    "    scaler_global=False,\n",
    "    epochs_max=40,\n",
    "    ocsvm_nu=0.110,\n",
    "    ocsvm_gamma=59.636,\n",
    ")\n",
    "\n",
    "# NAIVE_ROBUST (2D)\n",
    "# -----------------------------------------------------------\n",
    "NAIVE_ROBUST_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"NAIVE-ROBUST-2D\",\n",
    "    scaler=\"robust\",\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.05, # Decreased, to avoid \"all zeros\" prediction\n",
    "    optimizer_decay=0.002,\n",
    "    epochs_best=5,\n",
    "    ocsvm_nu=0.214,\n",
    "    ocsvm_gamma=2.354,\n",
    ")\n",
    "\n",
    "# VALID_ROBUST (2D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_2D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-ROBUST-2D\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=False,\n",
    "    epochs_best=6,\n",
    "    epochs_max=20,\n",
    "    optimizer=\"sgd\",\n",
    "    optimizer_lr=0.05,  # Decrease LR, to avoid \"all zeros\" prediction\n",
    "    optimizer_decay=0.002,\n",
    "    ocsvm_nu=0.190,\n",
    "    ocsvm_gamma=0.069,\n",
    ")\n",
    "\n",
    "# VALID_ROBUST (1D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_ROBUST_1D = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-ROBUST-1D\",\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=False,\n",
    "    model_variant=\"1d\", \n",
    "    filters=[32, 64, 128, 64],    \n",
    "    epochs_best=9,\n",
    "    epochs_max=20,\n",
    "    ocsvm_nu=0.156,\n",
    "    ocsvm_gamma=33.932,\n",
    ")\n",
    "\n",
    "# FCN_ROBUST (1D)\n",
    "# -----------------------------------------------------------\n",
    "VALID_FCN_ROBUST = dataclasses.replace(\n",
    "    NAIVE_MINMAX_2D,\n",
    "    name=\"VALID-FCN-ROBUST-FINAL\",\n",
    "    task_types=[2, 4, 6],\n",
    "    feature_cols=[\"acc_x\", \"acc_y\", \"acc_z\"], \n",
    "    frequency=25,\n",
    "    window_size=25*5,\n",
    "    step_width=25*5,\n",
    "    scaler=\"robust\",\n",
    "    scaler_global=False,\n",
    "    seconds_per_subject_train=60 * 10,\n",
    "    seconds_per_subject_test=60 * 10,\n",
    "    max_pairs_per_session=60 * 10,\n",
    "    model_variant=\"fcn\",\n",
    "    filters=[32, 64, 32],\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.001,\n",
    "    optimizer_decay=None,\n",
    "    batch_size=300,\n",
    "    margin=1,\n",
    "    epochs_best=40,\n",
    "    epochs_max=80,\n",
    "    ocsvm_nu=0.165,\n",
    "    ocsvm_gamma=8.296,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Select Approach <a id='1.4'>&nbsp;</a> \n",
    "Select the parameters to use for current notebook execution here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = NAIVE_MINMAX_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of current Experiment Parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_ppp(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2. Initial Data Preparation <a id='2'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset <a id='2.1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmog = DatasetLoader(\n",
    "    hdf5_file=HMOG_HDF5,\n",
    "    table_name=P.table_name,\n",
    "    max_subjects=P.max_subjects,\n",
    "    task_types=P.task_types,\n",
    "    exclude_subjects=P.exclude_subjects,\n",
    "    exclude_cols=EXCLUDE_COLS,\n",
    "    seed=SEED,\n",
    ")\n",
    "hmog.data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalize Features (if global) <a id='2.2'>&nbsp;</a> \n",
    "Used here for naive approach (before splitting into test and training sets). Otherwise it's used during generate_pairs() and respects train vs. test borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if P.scaler_global:\n",
    "    print(\"Normalize all data before splitting into train and test sets...\")\n",
    "    hmog.all, scalers = utils_custom_scale(\n",
    "        hmog.all,\n",
    "        scale_cols=P.feature_cols,        \n",
    "        feature_cols=P.feature_cols,\n",
    "        scaler_name=P.scaler,\n",
    "        scope=P.scaler_scope,\n",
    "        plot=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipped, normalize after splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Split Dataset for Valid/Test <a id='2.3'>&nbsp;</a> \n",
    "In two splits: one used during hyperparameter optimization, and one used during testing.\n",
    "\n",
    "The split is done along the subjects: All sessions of a single subject will either be in the validation split or in the testing split, never in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmog.split_train_valid_train_test(\n",
    "    n_valid_train=P.n_valid_train_subjects,\n",
    "    n_valid_test=P.n_valid_test_subjects,\n",
    "    n_test_train=P.n_test_train_subjects,\n",
    "    n_test_test=P.n_test_test_subjects,\n",
    ")\n",
    "hmog.data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Normalize features (if not global) <a id='2.4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not P.scaler_global:\n",
    "    print(\"Scaling Data for Siamese Network only...\")\n",
    "    print(\"Training Data:\")\n",
    "    hmog.valid_train, _ = utils_custom_scale(\n",
    "        hmog.valid_train,\n",
    "        scale_cols=P.feature_cols,\n",
    "        feature_cols=P.feature_cols,\n",
    "        scaler_name=P.scaler,\n",
    "        scope=P.scaler_scope,\n",
    "        plot=True,        \n",
    "    )\n",
    "    print(\"Validation Data:\")\n",
    "    hmog.valid_test, _ = utils_custom_scale(\n",
    "        hmog.valid_test,\n",
    "        scale_cols=P.feature_cols,        \n",
    "        feature_cols=P.feature_cols,\n",
    "        scaler_name=P.scaler,\n",
    "        scope=P.scaler_scope,\n",
    "        plot=True,        \n",
    "    )\n",
    "else:\n",
    "    print(\"Skipped, already normalized.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Check Splits <a id='2.5'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 2.6 Reshape Features  <a id='2.6'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape & cache Set for Training Siamese Network:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape & cache Set for Validating Siamese Network:** (also used to optimize OCSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape & cache Set for Training/Validation OCSVM:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshape & cache Set for Training/Testing OCSVM:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 3. Generate Scenario Pairs <a id='3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load cached Data <a id='3.1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siamese_train = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_train.msg\")\n",
    "df_siamese_valid = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_valid.msg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build positive/negative Pairs  <a id='3.2'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect Pairs <a id='3.3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cache Pairs <a id='3.4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 4. Siamese Network <a id='4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load cached Pairs <a id='4.1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siamese_train_pairs = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_train_pairs.msg\")\n",
    "df_siamese_valid_pairs = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_valid_pairs.msg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 4.2 Build Model <a id='4.2'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check contrastive loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Model with 2D Filters, as derived from Centeno et al. (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2d(input_shape, filters):\n",
    "    \"\"\"\n",
    "        Siamese CNN architecture with 3D input and 2D filters\n",
    "    \"\"\"\n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "    # Convolutional Neural Network\n",
    "    inputs = Input(input_shape, name=\"input\")\n",
    "    x = Conv2D(filters[0], (7, 7), padding=\"same\", activation=\"tanh\", name=\"conv1\")(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp1\")(x)\n",
    "    x = Conv2D(filters[1], (5, 5), padding=\"same\", activation=\"tanh\", name=\"conv2\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp2\")(x)\n",
    "    x = Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv3\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp3\")(x)\n",
    "    x = Conv2D(filters[3], (3, 3), padding=\"same\", activation=\"tanh\", name=\"conv4\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"same\", name=\"mp4\")(x)\n",
    "    x = Flatten(name=\"flat\")(x)\n",
    "    \n",
    "    # Basemodel instance\n",
    "    basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "    # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "    encoded_l = basemodel(left_inputs)\n",
    "    encoded_r = basemodel(right_inputs)\n",
    "\n",
    "    # Add a customized layer to compute the distance between the encodings\n",
    "    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "    # Combine into one net\n",
    "    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "    # return the model\n",
    "    return siamese_net, basemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Model with 1D Filters, similar than Centeno et al. (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1d(input_shape, filters):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "    # Convolutional Neural Network\n",
    "    inputs = Input(input_shape, name=\"input\")\n",
    "    x = Conv1D(filters[0], 7, activation=\"elu\", padding=\"same\", name=\"conv1\")(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"mp1\")(x)\n",
    "    x = Conv1D(filters[1], 5, activation=\"elu\", padding=\"same\", name=\"conv2\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"mp2\")(x)\n",
    "    x = Conv1D(filters[2], 3, activation=\"elu\", padding=\"same\", name=\"conv3\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"mp3\")(x)\n",
    "    x = Conv1D(filters[3], 3, activation=\"elu\", padding=\"same\", name=\"conv4\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"mp5\")(x)\n",
    "    x = Flatten(name=\"flat\")(x)\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "    # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "    encoded_l = basemodel(left_inputs)\n",
    "    encoded_r = basemodel(right_inputs)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "    # return the model\n",
    "    return siamese_net, basemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Model with FCN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fcn(input_shape, filters):\n",
    "    # Define the tensors for the two input images\n",
    "    left_inputs = Input(input_shape, name=\"left_inputs\")\n",
    "    right_inputs = Input(input_shape, name=\"right_inputs\")\n",
    "\n",
    "    # Convolutional Neural Network\n",
    "    inputs = Input(input_shape, name=\"input\")\n",
    "    x = Conv1D(\n",
    "        filters=filters[0],\n",
    "        kernel_size=8,\n",
    "        strides=1,\n",
    "        activation=None,\n",
    "        padding=\"same\",\n",
    "        name=\"conv1\",\n",
    "    )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.1, name=\"drop1\")(x)\n",
    "    x = Conv1D(\n",
    "        filters=filters[1],\n",
    "        kernel_size=5,\n",
    "        strides=1,\n",
    "        activation=None,\n",
    "        padding=\"same\",\n",
    "        name=\"conv2\",\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.1, name=\"drop2\")(x)\n",
    "    x = Conv1D(\n",
    "        filters=filters[2],\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        activation=None,\n",
    "        padding=\"same\",\n",
    "        name=\"conv3\",\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(32, activation=\"sigmoid\", name=\"dense\")(x) # <--- !!!!!!!!!!!!\n",
    "\n",
    "    # Basemodel instance\n",
    "    basemodel = Model(inputs, x, name=\"basemodel\")\n",
    "\n",
    "    # using same instance of \"basemodel\" to share weights between left/right networks\n",
    "    encoded_l = basemodel(left_inputs)\n",
    "    encoded_r = basemodel(right_inputs)\n",
    "\n",
    "    # Add a customized layer to compute the distance between the encodings\n",
    "    distance_layer = Lambda(k_euclidean_dist, name=\"distance\")([encoded_l, encoded_r])\n",
    "\n",
    "    # Combine into one net\n",
    "    siamese_net = Model(inputs=[left_inputs, right_inputs], outputs=distance_layer)\n",
    "\n",
    "    # return the model\n",
    "    return siamese_net, basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name, window_size, feature_cols, filters):\n",
    "    print(f\"Using Model variant {name}...\")\n",
    "    if name == \"1d\":\n",
    "        model, basemodel = build_model_1d((window_size, len(feature_cols)), filters)\n",
    "    elif name == \"2d\":\n",
    "        model, basemodel = build_model_2d((window_size, len(feature_cols), 1), filters)\n",
    "    elif name == \"fcn\":\n",
    "        model, basemodel = build_model_fcn((window_size, len(feature_cols)), filters)\n",
    "    else:\n",
    "        raise BaseException(\"Error: Not a valid model name: {1d, 2d, fcn}\")\n",
    "\n",
    "    return model, basemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect model architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare Features <a id='4.3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_X_y_pair(df):\n",
    "    \n",
    "    X_left = np.stack(list(df[\"left_X\"].values))\n",
    "    X_right = np.stack(list(df[\"right_X\"].values))\n",
    "    \n",
    "    X = [X_left, X_right]\n",
    "    y = df[\"label\"].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prep_X_y_pair(df_siamese_train_pairs)\n",
    "X_valid, y_valid = prep_X_y_pair(df_siamese_valid_pairs)\n",
    "\n",
    "# 2D Filter Model needs flat 4th dimension\n",
    "if P.model_variant == \"2d\":\n",
    "    X_train[0] = X_train[0].reshape((*X_train[0].shape, 1))\n",
    "    X_train[1] = X_train[1].reshape((*X_train[1].shape, 1))\n",
    "    X_valid[0] = X_valid[0].reshape((*X_valid[0].shape, 1))\n",
    "    X_valid[1] = X_valid[1].reshape((*X_valid[1].shape, 1))\n",
    "\n",
    "print(\n",
    "    f\"Training samples:   {y_train.shape[0]}, shape: {X_train[0].shape},\"\n",
    "    + f\" class balance: {np.unique(y_train, return_counts=True)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation samples: {y_valid.shape[0]}, shape: {X_valid[0].shape},\"\n",
    "    + f\" class balance: {np.unique(y_valid, return_counts=True)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Search optimal Epoch <a id='4.4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Custom Keras Callback function.\n",
    "    \n",
    "    Used to predict and plot distances for positive and negative pairs\n",
    "    after each n-th epoch, along with some 'classification' metrics. \n",
    "    'Classification' here means to ability to distinguish between positive \n",
    "    and negative pairs using a threshold for the distance.\n",
    "    \n",
    "    Arguments:\n",
    "        payload {tuple}           -- Datasets used for evaluation: (X_valid, y_valid, X_train, y_train)\n",
    "        epoch_evaluate_freq {int} -- Frequency for evaluation. After every n-th epoch, \n",
    "                                     the results are evaluated and printed\n",
    "        save_plots {boolean}      -- Do you want to save plots as PDF? Path is configured via global\n",
    "                                     parameter REPORT_PATH.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, payload, epoch_evaluate_freq=1, save_plots=False):\n",
    "\n",
    "        self.X_valid, self.y_valid, self.X_train, self.y_train = payload\n",
    "        self.save_plots = save_plots\n",
    "        self.epoch_evaluate_freq = epoch_evaluate_freq\n",
    "\n",
    "        # Do we have train and valid set?\n",
    "        self.sets = []\n",
    "        if self.X_train:\n",
    "            self.sets.append([self.X_train, self.y_train, \"Train\"])\n",
    "        if self.X_valid:\n",
    "            self.sets.append([self.X_valid, self.y_valid, \"Valid\"])\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        print(32 * \"=\" + f\"[ Initial State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        print(32 * \"=\" + f\"[ Final State ]\" + 32 * \"=\", end=\"\")\n",
    "        for X, y, desc in self.sets:\n",
    "            self.evaluate(X, y, logs, desc, -1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(32 * \"=\" + f\"[   Epoch {epoch}   ]\" + 32 * \"=\", end=\"\")\n",
    "        if epoch % self.epoch_evaluate_freq == 0:  # Evaluate only every n-th epoch\n",
    "            for X, y, desc in self.sets:\n",
    "                self.evaluate(X, y, logs, desc, epoch)\n",
    "        else:\n",
    "            print(f\"\\n{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\")\n",
    "\n",
    "    def evaluate(self, X, y, logs, desc, epoch):\n",
    "        # Predict\n",
    "        y_score = self.model.predict(X)\n",
    "        y_score_neg = y_score * -1  # lower distance means closer to positive class\n",
    "\n",
    "        # Calc Metrics\n",
    "        roc_val = metrics.roc_auc_score(y, y_score_neg)\n",
    "        eer_val, thres = utils_eer(y, y_score_neg, True)\n",
    "        y_pred = np.where(y_score_neg > thres, 1, 0)\n",
    "        acc_val = metrics.accuracy_score(y, y_pred)\n",
    "        f1_val = metrics.f1_score(y, y_pred)\n",
    "\n",
    "        print(\n",
    "            f\"\\n{desc.upper()}: roc_auc: {roc_val:.4f}, \"\n",
    "            + f\"eer: {eer_val:.4f}, thres: {thres*-1:.4f} => \"\n",
    "            + f\"acc: {acc_val:.4f}, f1: {f1_val:.4f}\\n\"\n",
    "            + f\"{ ', '.join([k + ': ' + f'{v:.3f}' for k,v in logs.items()]) }\"\n",
    "        )\n",
    "\n",
    "        # Plot distances\n",
    "        mask = np.where(y == 1, True, False)\n",
    "        dist_positive = y_score[mask]\n",
    "        dist_negative = y_score[~mask]\n",
    "        plt = utils_plot_distance_hist(\n",
    "            dist_positive, dist_negative, thres * -1, desc=desc, margin=P.margin\n",
    "        )\n",
    "\n",
    "        if self.save_plots:\n",
    "            utils_save_plot(\n",
    "                plt,\n",
    "                REPORT_PATH\n",
    "                / f\"buech2019-siamese-{P.name.lower()}-epoch-{epoch+1}-{desc.lower()}.pdf\",\n",
    "            )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(name, lr=None, decay=None):\n",
    "    if name == \"sgd\":\n",
    "        lr = lr if lr != None else 0.01\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = SGD(lr=lr, decay=decay)\n",
    "    elif name == \"adam\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        decay = decay if decay != None else 0\n",
    "        optimizer = Adam(lr=lr, decay=decay)\n",
    "    elif name == \"rmsprop\":\n",
    "        lr = lr if lr != None else 0.001\n",
    "        optimizer = RMSprop(lr=lr)\n",
    "    else:\n",
    "        raise BaseException(\"Error: Not a valid model name: 1d or 2d.\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4.5 Check Distances <a id='4.5'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Rebuild and train to optimal Epoch  <a id='4.6'>&nbsp;</a> \n",
    "Now, that we know the learning curve, we can rebuild the model and train it until the best Epoch.\n",
    "\n",
    "Also, we will include the validation data to have more training data. \n",
    "\n",
    "**Note:** This also means, that the training metrics are not valid anymore, because we don't have any validation data left to test against..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Cache model <a id='4.7'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 5. Visualize Deep Features <a id='5'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load cached Data <a id='5.1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_siamese_valid = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_valid.msg\")\n",
    "df_siamese_train = pd.read_msgpack(OUTPUT_PATH / \"df_siamese_train.msg\")\n",
    "\n",
    "df_ocsvm_train_valid = pd.read_msgpack(OUTPUT_PATH / \"df_ocsvm_train_valid.msg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Extract CNN from Siamese Model <a id='5.2'>&nbsp;</a> \n",
    "I do this by redirecting inputs and outputs.\n",
    "\n",
    "However, the network still needs a pair as input (I wasn't able to change this). This slows down a little bit the prediction (as the input is predicted twice), but doesn't change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deep_feature_model(model_path):\n",
    "    # Copy of function from above. It's just more convenient for partially \n",
    "    # executing the notebook.\n",
    "    def k_contrastive_loss(y_true, dist):\n",
    "        \"\"\"Contrastive loss from Hadsell-et-al.'06\n",
    "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "        \"\"\"\n",
    "        margin = P.margin\n",
    "        return K.mean(\n",
    "            y_true * K.square(dist)\n",
    "            + (1 - y_true) * K.square(K.maximum(margin - dist, 0))\n",
    "        )\n",
    "    \n",
    "    # Load Trained Siamese Network\n",
    "    model = load_model(\n",
    "        str(model_path.resolve()),\n",
    "        custom_objects={\"k_contrastive_loss\": k_contrastive_loss},\n",
    "    )\n",
    "\n",
    "    # Extract one of the child networks\n",
    "    deep_feature_model = Model(\n",
    "        inputs=model.get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "        outputs=model.get_layer(\"basemodel\").get_output_at(1),\n",
    "    )\n",
    "    \n",
    "    return deep_feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "deep_feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 5.3 Test Generation of Deep Features <a id='5.3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_X_y_single(df):\n",
    "    X = np.stack(list(df[\"X\"].values))\n",
    "    y = df[\"label\"].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sample_by_subject(df):\n",
    "    sample_by_subject = []\n",
    "    df[\"label\"] = 1\n",
    "    for subj in df[\"subject\"].unique():\n",
    "        df_subj = df[df[\"subject\"] == subj]\n",
    "        X_sub, y_sub = prep_X_y_single(df_subj)\n",
    "        sample_by_subject.append((X_sub, y_sub, subj))\n",
    "    return sample_by_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select subset (for plotting) and transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Deep Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize in 2D using PCA <a id='5.4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 6. OCSVM <a id='6'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Load cached Data <a id='6.1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocsvm_train_valid = pd.read_msgpack(OUTPUT_PATH / \"df_ocsvm_train_valid.msg\")\n",
    "df_ocsvm_train_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load trained Siamese Model <a id='6.2'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper methods to load model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deep_feature_model(model_path):\n",
    "    warnings.filterwarnings(\"ignore\")  # Silence depr. warnings\n",
    "\n",
    "    # Copy of function from above. It's just more convenient for partially executing the notebook.\n",
    "    def k_contrastive_loss(y_true, dist):\n",
    "        \"\"\"Contrastive loss from Hadsell-et-al.'06\n",
    "        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "        \"\"\"\n",
    "        margin = P.margin\n",
    "        return K.mean(\n",
    "            y_true * K.square(dist)\n",
    "            + (1 - y_true) * K.square(K.maximum(margin - dist, 0))\n",
    "        )\n",
    "\n",
    "    # Load Trained Siamese Network\n",
    "    model = load_model(\n",
    "        str(model_path.resolve()),\n",
    "        custom_objects={\"k_contrastive_loss\": k_contrastive_loss},\n",
    "    )\n",
    "\n",
    "    # Extract one of the child networks\n",
    "    deep_feature_model = Model(\n",
    "        inputs=model.get_input_at(0),  # get_layer(\"left_inputs\").input,\n",
    "        outputs=model.get_layer(\"basemodel\").get_output_at(1),\n",
    "    )\n",
    "\n",
    "    return deep_feature_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Search for Parameters <a id='6.3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocsvm_train_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 6.4 Inspect Search Results <a id='6.4'>&nbsp;</a> \n",
    "**Raw Results & Stats:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_for_pyod (y) : \n",
    "    for i in range(len(y)): \n",
    "        if y[i] == -1: \n",
    "            y[i]=1\n",
    "        elif y[i] == 1:\n",
    "            y[i] = 0 \n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.sod import SOD\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "param_dist = {}#\"contamination\" :   np.linspace(.5, 0,6)}\n",
    "\n",
    "# Load Siamese CNN Model\n",
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "\n",
    "\n",
    "modelss = [\n",
    "    ABOD(), \n",
    "    CBLOF(), \n",
    "    FeatureBagging(), \n",
    "    HBOS(), \n",
    "    IForest(), \n",
    "    KNN(), \n",
    "    LOF(), \n",
    "    LOCI(), \n",
    "    MCD(), \n",
    "    OCSVM(), \n",
    "    PCA(), \n",
    "    SOS(), \n",
    "    #LSCP(), \n",
    "    COF(), \n",
    "    SOD(), \n",
    "]\n",
    "for mooo in modelss: \n",
    "\n",
    "    df_results = None  # Will be filled with randomsearch scores\n",
    "    for run in tqdm(range(1)):\n",
    "        for df_cv_scenarios, owner, impostors in tqdm(\n",
    "            utils_generate_cv_scenarios(\n",
    "                df_ocsvm_train_valid,\n",
    "                samples_per_subject_train=P.samples_per_subject_train,\n",
    "                samples_per_subject_test=P.samples_per_subject_test,\n",
    "                seed=SEED + run,\n",
    "                scaler=P.scaler,\n",
    "                scaler_global=P.scaler_global,\n",
    "                scaler_scope=P.scaler_scope,\n",
    "                deep_model=deep_feature_model,\n",
    "                model_variant=P.model_variant,\n",
    "                feature_cols=P.feature_cols,\n",
    "            ),\n",
    "            desc=\"Owner\",\n",
    "            total=df_ocsvm_train_valid[\"subject\"].nunique(),\n",
    "            leave=False,\n",
    "        ):\n",
    "\n",
    "\n",
    "            X = np.array(df_cv_scenarios[\"X\"].values.tolist())\n",
    "            y = df_cv_scenarios[\"label\"].values\n",
    "            y = trans_for_pyod(y)\n",
    "            train_valid_cv = utils_create_cv_splits(df_cv_scenarios[\"mask\"].values, SEED)\n",
    "\n",
    "            model = mooo\n",
    "\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            random_search = RandomizedSearchCV(\n",
    "                model,\n",
    "                param_distributions=param_dist,\n",
    "                cv=train_valid_cv,\n",
    "                n_iter=80,\n",
    "                n_jobs=CORES,\n",
    "                refit=False,\n",
    "                scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "                verbose=0,\n",
    "                return_train_score=False,\n",
    "                iid=False,\n",
    "                error_score=np.nan,\n",
    "                random_state=SEED,\n",
    "            )\n",
    "\n",
    "            random_search.fit(X, y)\n",
    "\n",
    "            df_report = utils_cv_report(random_search, owner, impostors)\n",
    "            df_report[\"run\"] = run\n",
    "            df_results = pd.concat([df_results, df_report], sort=False)\n",
    "\n",
    "    df_results.to_csv(OUTPUT_PATH / f\"{P.name+str(mooo)[:10]}_ez_All_MODELS_random_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mooo in modelss: \n",
    "    print(str(mooo)[:10])\n",
    "    df_results=pd.read_csv(OUTPUT_PATH /f\"{P.name+str(mooo)[:10]}_All_MODELS_random_search_results.csv\")\n",
    "    print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "    display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"owner\",\n",
    "                \"rank_test_eer\",\n",
    "                \"mean_test_eer\",\n",
    "                \"std_test_eer\",\n",
    "                \"mean_test_accuracy\",\n",
    "                \"std_test_accuracy\",\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdvddvgdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best in naive \n",
    "encoder_architectures = [\n",
    "    [16,1],\n",
    "]\n",
    "x=\"NAIVE-MINMAX-2D\"\n",
    "for enc in encoder_architectures[::-1]: \n",
    "    if os.path.isfile(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez_yes_params_in_construct__random_search_results_again.csv\"): \n",
    "        print(enc)\n",
    "        df_results = pd.read_csv(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez_yes_params_in_construct__random_search_results_again.csv\")\n",
    "        display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "                'param_optimizer', 'param_dropout_rate', 'param_contamination', 'param_capacity', 'param_batch_size',\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(3) )\n",
    "        \n",
    "# VAE(enc,dec,output_activation=activations.softmax,optimizer='sgd',loss=losses.mse,l2_regularizer=.5,hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,contamination=.1,capacity=1,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best in valid \n",
    "encoder_architectures = [\n",
    "    [32,16,1],\n",
    "]\n",
    "x=\"VALID-FCN-ROBUST-FINAL\"\n",
    "for enc in encoder_architectures[::-1]: \n",
    "    if os.path.isfile(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez__random_search_results_again.csv\"): \n",
    "        print(enc)\n",
    "        df_results = pd.read_csv(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez__random_search_results_again.csv\")\n",
    "        display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"owner\",\n",
    "                \"mean_test_eer\",\n",
    "                \"mean_test_accuracy\",\n",
    "                'param_optimizer', 'param_dropout_rate', 'param_contamination', 'param_capacity', 'param_batch_size'\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.vae import VAE\n",
    "from keras import losses\n",
    "from keras import activations\n",
    "\n",
    "param_dist = {\n",
    "                #\"l2_regularizer\" :  np.linspace(1, 0,5) ,\n",
    "                \"contamination\" :   [0.15, 0.133, 0.125, .1 ] ,\n",
    "                \"dropout_rate\" :  [.25] ,\n",
    "                \"optimizer\" : ['sgd'], \n",
    "    \n",
    "                \"batch_size\" :  [32],#,16,64],\n",
    "                #\"epochs\" : np.linspace(600, 100,3),\n",
    "                #\"gamma\" : np.linspace(2, 0,5) , \n",
    "                \"capacity\" : [1] , \n",
    "                #\"loss\": [losses.mse, losses.msle, losses.poisson, losses.mape, losses.mae, losses.kld] , \n",
    "                #\"output_activation\": [ activations.relu, activations.sigmoid, activations.softmax, activations.softplus,\n",
    "                #                      activations.softsign, activations.tanh, activations.selu, activations.elu, activations.exponential],\n",
    "                #\"hidden_activation\": [ activations.relu, activations.sigmoid, activations.softmax, activations.softplus,\n",
    "                #                      activations.softsign, activations.tanh, activations.selu, activations.elu, activations.exponential]\n",
    "\n",
    "                 }\n",
    "\n",
    "\n",
    "# Load Siamese CNN Model\n",
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "\n",
    "encoder_architectures = [\n",
    "  #  [32,24,16,8,4,2,1],\n",
    "    \n",
    "    [32,2],\n",
    "    [32,1],\n",
    "    [16,1],\n",
    "  #  [64,1],\n",
    "##    [32,16,8,4,2],\n",
    "##    [32,16,8,4],\n",
    " #   [32,16,8,2],\n",
    "    [32,16,1],\n",
    "    [32,16,8,1],\n",
    "    [32,16,8,4,1],\n",
    "    [32,16,2,1],\n",
    "    [32,16,8,4,2,1],\n",
    "]\n",
    "#all_df_reports=[] \n",
    "for enc in encoder_architectures[::-1]: \n",
    "    df_results = None  # Will be filled with randomsearch scores\n",
    "    for run in tqdm(range(1)):\n",
    "        for df_cv_scenarios, owner, impostors in tqdm(\n",
    "            utils_generate_cv_scenarios(\n",
    "                df_ocsvm_train_valid,\n",
    "                samples_per_subject_train=P.samples_per_subject_train,\n",
    "                samples_per_subject_test=P.samples_per_subject_test,\n",
    "                seed=SEED + run,\n",
    "                scaler=P.scaler,\n",
    "                scaler_global=P.scaler_global,\n",
    "                scaler_scope=P.scaler_scope,\n",
    "                deep_model=deep_feature_model,\n",
    "                model_variant=P.model_variant,\n",
    "                feature_cols=P.feature_cols,\n",
    "            ),\n",
    "            desc=\"Owner\",\n",
    "            total=df_ocsvm_train_valid[\"subject\"].nunique(),\n",
    "            leave=False,\n",
    "        ):\n",
    "\n",
    "            X = np.array(df_cv_scenarios[\"X\"].values.tolist())\n",
    "            y = df_cv_scenarios[\"label\"].values\n",
    "            y=trans_for_pyod(y)\n",
    "\n",
    "            train_valid_cv = utils_create_cv_splits(df_cv_scenarios[\"mask\"].values, SEED)\n",
    "\n",
    "            dec=enc[::-1]\n",
    "\n",
    "            print(      enc,dec)\n",
    "            model = VAE(enc,dec,\n",
    "                           output_activation=activations.softmax,optimizer='adam',loss=losses.mse,l2_regularizer=.5,\n",
    "                           hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,\n",
    "                           contamination=.5,capacity=1,batch_size=32) \n",
    "\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            random_search = RandomizedSearchCV(\n",
    "                model,\n",
    "                param_distributions=param_dist,\n",
    "                cv=train_valid_cv,\n",
    "                n_iter=80,\n",
    "                n_jobs=CORES,\n",
    "                refit=False,\n",
    "                scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "                verbose=0,\n",
    "                return_train_score=False,\n",
    "                iid=False,\n",
    "                error_score=np.nan,\n",
    "                random_state=SEED,\n",
    "            )\n",
    "\n",
    "            random_search.fit(X, y)\n",
    "\n",
    "            df_report = utils_cv_report(random_search, owner, impostors)\n",
    "            df_report[\"run\"] = run\n",
    "            df_results = pd.concat([df_results, df_report], sort=False)\n",
    "        #all_df_reports.append([enc,df_results])\n",
    "        df_results.to_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE_ez_yes_params_in_construct__random_search_results_again.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best results for each owner: transofrm Y to 0/1 instd -1/1 & contam is .5 32,2\")\n",
    "\n",
    "\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#36 16 2\n",
    "#df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "print(\"Best results for each owner: transofrm Y to 0/1 instd -1/1 & contam is .5 32,2\")\n",
    "print('''                          output_activation=activations.softmax,optimizer='adam',loss=losses.mse,l2_regularizer=.5,\n",
    "                           hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,\n",
    "                           contamination=.5,capacity=1,batch_size=32) ''')\n",
    "\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_reports[5][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best results for each owner: transofrm Y to 0/1 instd -1/1 \")\n",
    "print('''     new param search ''')\n",
    "for enc in encoder_architectures[::-1]: \n",
    "    df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE_ez__random_search_results_again.csv\")\n",
    "    print(\"Best results for each owner:\")\n",
    "\n",
    "    display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"owner\",\n",
    "                \"rank_test_eer\",\n",
    "                \"mean_test_eer\",\n",
    "                \"std_test_eer\",\n",
    "                \"mean_test_accuracy\",\n",
    "                \"std_test_accuracy\",\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(10)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "    display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"mean_fit_time\",\n",
    "                \"mean_test_accuracy\",\n",
    "                \"std_test_accuracy\",\n",
    "                \"mean_test_eer\",\n",
    "                \"std_test_eer\",\n",
    "            ]\n",
    "        ].describe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "print(\"Best results for each owner: transofrm Y to 0/1 instd -1/1 & contam is .5 32,2\")\n",
    "print('''                          output_activation=activations.softmax,optimizer='adam',loss=losses.mse,l2_regularizer=.5,\n",
    "                           hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,\n",
    "                           contamination=.5,capacity=1,batch_size=32) ''')\n",
    "\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "print(\"Best results for each owner: transofrm Y to 0/1 instd -1/1 contam is .25\")\n",
    "'''                          output_activation=activations.softmax,optimizer='adam',loss=losses.mse,l2_regularizer=.5,\n",
    "                           hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,\n",
    "                           contamination=.5,capacity=1,batch_size=32) '''\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "print(\"Best results for each owner: WHEN EER func does ypred*-1\")\n",
    "\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "print(\"Best results for each owner: WHEN 1-PREDICT\")\n",
    "\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "        ]\n",
    "    ].sort_values(\"mean_test_eer\").head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "display(\n",
    "    df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "        [\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try xgbod \n",
    "# try OCSVM (best is .13) and LOF (best was .15) from sklearn vs pyod :/ \n",
    "https://github.com/yzhao062/Pyod/issues/4\n",
    "he says they r the same but instead of outliers having lower scores they have higher scores \n",
    "# try all pyod models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(OUTPUT_PATH / f\"{P.name+str(mooo)[:10]}_All_MODELS_random_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for enc in encoder_architectures[::-1]: \n",
    "    df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name+str(enc)}__VAE__random_search_results.csv\")\n",
    "    print(\"Best results for each owner:\")\n",
    "\n",
    "    display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"owner\",\n",
    "                \"rank_test_eer\",\n",
    "                \"mean_test_eer\",\n",
    "                \"std_test_eer\",\n",
    "                \"mean_test_accuracy\",\n",
    "                \"std_test_accuracy\",\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(10)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\\nMost relevant statistics:\")\n",
    "    display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "                \"mean_fit_time\",\n",
    "                \"mean_test_accuracy\",\n",
    "                \"std_test_accuracy\",\n",
    "                \"mean_test_eer\",\n",
    "                \"std_test_eer\",\n",
    "            ]\n",
    "        ].describe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot parameters of top n of 30 results for every Owner:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils_plot_randomsearch_results(df_results, 1)\n",
    "utils_save_plot(plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-parameters.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing <a id='7'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load cached Data <a id='7.1'>&nbsp;</a> \n",
    "During testing, a split with different users than used for hyperparameter optimization is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocsvm_train_test = pd.read_msgpack(OUTPUT_PATH / \"df_ocsvm_train_test.msg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Evaluate Authentication Performance <a id='7.2'>&nbsp;</a> \n",
    "- Using Testing Split, Scenario Cross Validation, and multiple runs to lower impact of random session/sample selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Siamese CNN Model\n",
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "modelss = [\n",
    "    ABOD(), \n",
    "    CBLOF(), \n",
    "    FeatureBagging(), \n",
    "    HBOS(), \n",
    "    IForest(), \n",
    "    KNN(), \n",
    "    LOF(), \n",
    "    LOCI(), \n",
    "    MCD(), \n",
    "    OCSVM(), \n",
    "    PCA(), \n",
    "    SOS(), \n",
    "    #LSCP(), \n",
    "    COF(), \n",
    "    SOD(), \n",
    "]\n",
    "for mooo in modelss: \n",
    "    df_results = None  # Will be filled with cv scores\n",
    "    for i in tqdm(range(5), desc=\"Run\", leave=False):  # Run whole test 5 times\n",
    "        for df_cv_scenarios, owner, impostors in tqdm(\n",
    "            utils_generate_cv_scenarios(\n",
    "                df_ocsvm_train_test,\n",
    "                samples_per_subject_train=P.samples_per_subject_train,\n",
    "                samples_per_subject_test=P.samples_per_subject_test,\n",
    "                seed=SEED,\n",
    "                scaler=P.scaler,\n",
    "                scaler_global=P.scaler_global,\n",
    "                scaler_scope=P.scaler_scope,\n",
    "                deep_model=deep_feature_model,\n",
    "                model_variant=P.model_variant,\n",
    "                feature_cols=P.feature_cols,\n",
    "            ),\n",
    "            desc=\"Owner\",\n",
    "            total=df_ocsvm_train_test[\"subject\"].nunique(),\n",
    "            leave=False,\n",
    "        ):\n",
    "\n",
    "            X = np.array(df_cv_scenarios[\"X\"].values.tolist())\n",
    "            y = df_cv_scenarios[\"label\"].values\n",
    "            y = trans_for_pyod(y)\n",
    "\n",
    "            train_test_cv = utils_create_cv_splits(df_cv_scenarios[\"mask\"].values, SEED)\n",
    "\n",
    "            model = mooo #VAE([16,1],[1,16],output_activation=activations.softmax,optimizer='sgd',loss=losses.mse,l2_regularizer=.5,hidden_activation=activations.tanh,gamma=1.5,dropout_rate=.25,epochs=400,contamination=.1,capacity=1,batch_size=32)\n",
    "\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = cross_validate(\n",
    "                model,\n",
    "                X,\n",
    "                y,\n",
    "                cv=train_test_cv,\n",
    "                scoring={\"eer\": utils_eer_scorer, \"accuracy\": \"accuracy\"},\n",
    "                n_jobs=CORES,\n",
    "                verbose=1,\n",
    "                return_train_score=True,\n",
    "            )\n",
    "            df_score = pd.DataFrame(scores)\n",
    "            df_score[\"owner\"] = owner\n",
    "            df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n",
    "            df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "            df_results = pd.concat([df_results, df_score], axis=0)\n",
    "\n",
    "    df_results.to_csv(OUTPUT_PATH / f\"{P.name+mooo}_ALLMODELS_test_results.csv\", index=False)\n",
    "    df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Results from \"EER & Accuracy\" evaluation & prepare for plotting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name}_VAE_test_results.csv\")\n",
    "df_plot = df_results.rename(\n",
    "    columns={\"test_accuracy\": \"Test Accuracy\", \"test_eer\": \"Test EER\", \"owner\": \"Owner\"}\n",
    ").astype({\"Owner\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"NAIVE-MINMAX-2D\"\n",
    "for enc in encoder_architectures[::-1]: \n",
    "    if os.path.isfile(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez_yes_params_in_construct__random_search_results_again.csv\"): \n",
    "        print(enc)\n",
    "        df_results = pd.read_csv(OUTPUT_PATH / f\"{x+str(enc)}__VAE_ez_yes_params_in_construct__random_search_results_again.csv\")\n",
    "        display(\n",
    "        df_results[df_results[\"rank_test_eer\"] <= 1][\n",
    "            [\n",
    "            \"owner\",\n",
    "            \"rank_test_eer\",\n",
    "            \"mean_fit_time\",\n",
    "            \"mean_test_eer\",\n",
    "            \"std_test_eer\",\n",
    "            \"mean_test_accuracy\",\n",
    "            \"std_test_accuracy\",\n",
    "            ]\n",
    "        ].sort_values(\"mean_test_eer\").head(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"NAIVE-MINMAX-2D\"\n",
    "for enc in encoder_architectures[::-1]: \n",
    "    if os.path.isfile(OUTPUT_PATH / f\"{P.name}_VAE_test_results.csv\"): \n",
    "        df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name}_VAE_test_results.csv\")\n",
    "        display(\n",
    "        df_results.sort_values(\"test_eer\", ascending=True).head(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Distribution of Accuracy per subject:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = utils_plot_acc_eer_dist(df_plot, \"Test Accuracy\")\n",
    "utils_save_plot(plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-acc.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Distribution of EER per subject:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = utils_plot_acc_eer_dist(df_plot, \"Test EER\") # with new archi [32,16,4,1],[1,4,16,32] instead of [32,16,8,4,1],[1,4,8,16,32]\n",
    "utils_save_plot(plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-eer.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = utils_plot_acc_eer_dist(df_plot, \"mean_test_eer\")\n",
    "utils_save_plot(plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-eer.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Evaluate increasing Training Set Size (Training Delay) <a id='7.3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_sizes = [1, 2, 3, 4, 10, 30, 60, 90, 125, 175, 250, 375]  # In samples\n",
    "\n",
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "\n",
    "df_results = None  # Will be filled with cv scores\n",
    "for i in tqdm(range(5), desc=\"Run\", leave=False):  # Run whole test 5 times\n",
    "    for n_train_samples in tqdm(training_set_sizes, desc=\"Train Size\", leave=False):\n",
    "        for df_cv_scenarios, owner, impostors in tqdm(\n",
    "            utils_generate_cv_scenarios(\n",
    "                df_ocsvm_train_test,\n",
    "                samples_per_subject_train=P.samples_per_subject_train,\n",
    "                samples_per_subject_test=P.samples_per_subject_test,\n",
    "                limit_train_samples=n_train_samples,  # samples overall\n",
    "                seed=SEED + i,\n",
    "                scaler=P.scaler,\n",
    "                scaler_global=P.scaler_global,\n",
    "                scaler_scope=P.scaler_scope,\n",
    "                deep_model=deep_feature_model,\n",
    "                model_variant=P.model_variant,\n",
    "                feature_cols=P.feature_cols,\n",
    "            ),\n",
    "            desc=\"Owner\",\n",
    "            total=df_ocsvm_train_test[\"subject\"].nunique(),\n",
    "            leave=False,\n",
    "        ):\n",
    "            X = np.array(df_cv_scenarios[\"X\"].values.tolist())\n",
    "            y = df_cv_scenarios[\"label\"].values\n",
    "\n",
    "            train_test_cv = utils_create_cv_splits(df_cv_scenarios[\"mask\"].values, SEED)\n",
    "\n",
    "            model = VAE([32,16,8,4,1],[1,4,8,16,32], contamination=.5, optimizer='sgd',                \n",
    "                gamma=3 , \n",
    "                capacity=.5 , \n",
    "                loss=losses.kld,\n",
    "                batch_size=32, \n",
    "                dropout_rate=.25,\n",
    "                epochs=200,\n",
    "                output_activation=activations.selu, \n",
    "                hidden_activation=activations.sigmoid, \n",
    "    )\n",
    "\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = cross_validate(\n",
    "                model,\n",
    "                X,\n",
    "                y,\n",
    "                cv=train_test_cv,\n",
    "                scoring={\"eer\": utils_eer_scorer},\n",
    "                n_jobs=CORES,\n",
    "                verbose=0,\n",
    "                return_train_score=True,\n",
    "            )\n",
    "            df_score = pd.DataFrame(scores)\n",
    "            df_score[\"owner\"] = owner\n",
    "            df_score[\"train_samples\"] = n_train_samples\n",
    "            df_score[\"train_eer\"] = df_score[\n",
    "                \"train_eer\"\n",
    "            ].abs()  # Revert scorer's signflip\n",
    "            df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "            df_results = pd.concat([df_results, df_score], axis=0)\n",
    "\n",
    "df_results.to_csv(OUTPUT_PATH / f\"{P.name}_train_delay_results.csv\", index=False)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Results from \"Training set size\" evaluation & prepare for plotting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name}_train_delay_results.csv\")\n",
    "df_plot = (\n",
    "    df_results[[\"test_eer\", \"owner\", \"train_samples\"]]\n",
    "    .groupby([\"owner\", \"train_samples\"], as_index=False)\n",
    "    .mean()\n",
    "    .astype({\"owner\": \"category\"})\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"test_eer\": \"Test EER\",\n",
    "            \"owner\": \"Owner\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "df_plot[\"Training Data in Seconds\"] = df_plot[\"train_samples\"] * P.window_size / P.frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot EER with increasing number of training samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_plot_training_delay(df_plot)\n",
    "utils_save_plot(plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-train-size.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Evaluate increasing Test Set Sizes (Detection Delay)<a id='7.4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Siamese CNN Model\n",
    "deep_feature_model = load_deep_feature_model(OUTPUT_PATH / f\"{P.name}_model.h5\")\n",
    "\n",
    "df_results = None  # Will be filled with cv scores\n",
    "for i in tqdm(range(50), desc=\"Run\", leave=False):  # Run whole test 5 times\n",
    "    for df_cv_scenarios, owner, impostors in tqdm(\n",
    "        utils_generate_cv_scenarios(\n",
    "            df_ocsvm_train_test,\n",
    "            samples_per_subject_train=P.samples_per_subject_train,\n",
    "            samples_per_subject_test=P.samples_per_subject_test,\n",
    "            limit_test_samples=1,  # Samples overall\n",
    "            seed=SEED + i,\n",
    "            scaler=P.scaler,\n",
    "            scaler_global=P.scaler_global,\n",
    "            scaler_scope=P.scaler_scope,\n",
    "            deep_model=deep_feature_model,\n",
    "            model_variant=P.model_variant,\n",
    "            feature_cols=P.feature_cols,\n",
    "        ),\n",
    "        desc=\"Owner\",\n",
    "        total=df_ocsvm_train_test[\"subject\"].nunique(),\n",
    "        leave=False,\n",
    "    ):\n",
    "        X = np.array(df_cv_scenarios[\"X\"].values.tolist())\n",
    "        y = df_cv_scenarios[\"label\"].values\n",
    "\n",
    "        train_test_cv = utils_create_cv_splits(df_cv_scenarios[\"mask\"].values, SEED)\n",
    "\n",
    "        model = VAE([32,16,8,4,1],[1,4,8,16,32], contamination=.5, optimizer='sgd',                \n",
    "                gamma=3 , \n",
    "                capacity=.5 , \n",
    "                loss=losses.kld,\n",
    "                batch_size=32, \n",
    "                dropout_rate=.25,\n",
    "                epochs=200,\n",
    "                output_activation=activations.selu, \n",
    "                hidden_activation=activations.sigmoid, \n",
    "    )\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        scores = cross_validate(\n",
    "            model,\n",
    "            X,\n",
    "            y,\n",
    "            cv=train_test_cv,\n",
    "            scoring={\"eer\": utils_eer_scorer},\n",
    "            n_jobs=CORES,\n",
    "            verbose=0,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "        df_score = pd.DataFrame(scores)\n",
    "        df_score[\"owner\"] = owner\n",
    "        df_score[\"run\"] = i\n",
    "        df_score[\"train_eer\"] = df_score[\"train_eer\"].abs()  # Revert scorer's signflip\n",
    "        df_score[\"test_eer\"] = df_score[\"test_eer\"].abs()\n",
    "        df_results = pd.concat([df_results, df_score], axis=0)\n",
    "\n",
    "df_results.to_csv(OUTPUT_PATH / f\"{P.name}_detect_delay_results.csv\", index=False)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Results from \"Detection Delay\" evaluation & prepare for plotting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(OUTPUT_PATH / f\"{P.name}_detect_delay_results.csv\")\n",
    "df_results[\"owner\"] = df_results[\"owner\"].astype(str)\n",
    "df_plot = df_results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Expanding Mean EER and confidence interval:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_plot_detect_delay(df_plot, factor=P.window_size / P.frequency, xlim=160)\n",
    "utils_save_plot(\n",
    "    plt, REPORT_PATH / f\"buech2019-siamese-{P.name.lower()}-detection-delay.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contauth",
   "language": "python",
   "name": "contauth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
