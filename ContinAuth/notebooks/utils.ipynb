{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Utility Functions\n",
    "**Description**\n",
    "\n",
    "This notebook contains shared functions used in other notebook. \n",
    "\n",
    "- *Plotting* and *Reporting/printing* related functions are mostly cosmetical to generate nice plots for the thesis and provide easy readable output.\n",
    "- The function that are *Scoring* related are important, as they are used to calculate the evaluation metrics. \n",
    "- The *Data Preparation* related functions are also very important, as they include the sliding window logic, and the (quite complex) normalization & cv scenario creation logic.\n",
    "\n",
    "The functions are followed by a *Check function* cell, which is used to demonstrated the different functions for transparency and sanity check their logic. This transparency, by demonstrating the results right next to the code is also the reason, why I implemented these functions as a Jupyter Notebook instead as a Python Module (where it would be better placed for more productive scenarios)\n",
    "\n",
    "**Usage**\n",
    "\n",
    "1. To make the functions in this notebook available in another notebook, run the following line in the consuming notebook:\n",
    "```\n",
    "%run utils.ipynb\n",
    "```\n",
    "\n",
    "\n",
    "2. To investigate the functions inside this notebook, enable the testing output by setting in the configuration section ([1. Preparations](#1)):\n",
    "```\n",
    "TEST_MODE = True\n",
    "```\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "**1 - [Preparations](#1)**  \n",
    "**2 - [Plotting related](#2)**  \n",
    "**3 - [Reporting/printing related](#3)**  \n",
    "**4 - [Scoring related](#4)**  \n",
    "**5 - [Data Preparation related](#5)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 1. Preparations <a id='1'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Extra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import asdict\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, make_scorer, auc\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.utils import resample as sk_resample\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import animation, rc\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Configuration\n",
    "Only relevant for sanity checking function in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODE = False  # Set to \"True\" to perform sanity checks, set to \"False\" before importing this notebook into others\n",
    "MAGENTA = (202/255, 18/255, 125/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2. Plotting related <a id='2'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_save_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_save_plot(fig, filepath):\n",
    "    \"\"\"Save plot to file using certain layout and dpi.\"\"\"\n",
    "    fig.savefig(filepath, bbox_inches=\"tight\", pad_inches=0.01, dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    plt.plot([1, 3, 2, 4])\n",
    "    \n",
    "    TEST_OUTPUT_PATH = Path.cwd() / \"output\" / \"utils\"\n",
    "    TEST_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    utils_save_plot(plt, TEST_OUTPUT_PATH / \"utils_save_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_set_output_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_set_output_style():\n",
    "    \"\"\"Set styles for matplotlib charts and pandas tables.\"\"\"\n",
    "\n",
    "    # Charts\n",
    "    # for seaborn:\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    sns.set(font=\"sans\")\n",
    "    sns.set_palette(\"tab10\")\n",
    "    # for plain matplotlib:\n",
    "    plt.style.use([\"seaborn-darkgrid\", \"seaborn-paper\"])\n",
    "    plt.rc(\"font\", family=\"sans\", size=8)\n",
    "    plt.rc(\"axes\", titlesize=6)\n",
    "    plt.rc(\"axes\", labelsize=6)\n",
    "    plt.rc(\"xtick\", labelsize=6)\n",
    "    plt.rc(\"ytick\", labelsize=6)\n",
    "    plt.rc(\"xtick.major\", pad=1)\n",
    "    plt.rc(\"ytick.major\", pad=3)\n",
    "    plt.rc(\"legend\", fontsize=6)\n",
    "    plt.rc(\"figure\", titlesize=6)\n",
    "\n",
    "    # Tables\n",
    "    pd.set_option(\"display.max_rows\", 500)\n",
    "    pd.set_option(\"display.max_columns\", 500)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    utils_set_output_style()\n",
    "    plt.plot([1, 3, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_boxplot_style &lt;dict&gt;, utils_lineplot_style &lt;dict&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a style I use a lot for boxplots:\n",
    "utils_boxplot_style = dict(\n",
    "    color=\"tab:blue\",\n",
    "    linewidth=0.5,\n",
    "    saturation=1,\n",
    "    width=0.7,\n",
    "    flierprops=dict(\n",
    "        marker=\"o\", markersize=2, markerfacecolor=\"none\", markeredgewidth=0.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define a style I use a lot for lineplots:\n",
    "utils_lineplot_style = dict(\n",
    "    color=\"tab:blue\", linewidth=0.5, marker=\"o\", markersize=3, markeredgewidth=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    utils_set_output_style()\n",
    "    fig = plt.figure(dpi=180, figsize=(5.473, 2))\n",
    "    sns.boxplot(\n",
    "        x=[\"Dist 1\", \"Dist 2\"],\n",
    "        y=[[2, 4, 3, 4, 15, 8, 3, 0, 2, 21], [12, 14, 13, 17, 15, 8, 11, 0, 2, 21]],\n",
    "        **utils_boxplot_style\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_randomsearch_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_randomsearch_results(df_results, n_top=1):\n",
    "    # Prepare data for plotting\n",
    "    df_plot = df_results[df_results[\"rank_test_eer\"] <= n_top].rename(\n",
    "        columns={\n",
    "            \"param_epochs\": r\"$\\nu$\",\n",
    "            \"mean_test_accuracy\": \"Mean Test Acc.\",\n",
    "            \"mean_test_eer\": \"Mean Test EER\",\n",
    "        }\n",
    "    )\n",
    "    df_plot[\"Mean Test EER\"] = df_plot[\"Mean Test EER\"] * -1  # Because fewer is more\n",
    "\n",
    "    median_nu = df_plot[r\"$\\nu$\"].median()\n",
    "    median_gamma = df_plot[r\"$\\gamma$\"].median()\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(5.473 / 1.3, 2), dpi=180)\n",
    "    g = sns.scatterplot(\n",
    "        x=r\"$\\nu$\",\n",
    "        y=r\"$\\gamma$\",\n",
    "        data=df_plot,\n",
    "        size=\"Mean Test EER\",\n",
    "        sizes=(7, 60),\n",
    "        hue=\"Mean Test EER\",\n",
    "        alpha=1,\n",
    "        #        palette=\"Blues\",\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Format Legend labels\n",
    "    leg = g.get_legend()\n",
    "    new_handles = [h for h in leg.legendHandles]\n",
    "    new_labels = []\n",
    "    for i, handle in enumerate(leg.legendHandles):\n",
    "        label = handle.get_label()\n",
    "        try:\n",
    "            new_labels.append(f\"{abs(float(label)):.3f}\")\n",
    "        except ValueError:\n",
    "            new_labels.append(\"\")\n",
    "\n",
    "    # Plot mean values\n",
    "    plt.plot(\n",
    "        [-0.01, 0.31],\n",
    "        [median_gamma, median_gamma],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.7 ** 2,\n",
    "        r\"median($\\gamma$)\",\n",
    "        fontsize=6,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.23,\n",
    "        median_gamma * 1.2 ** 2,\n",
    "        f\"{median_gamma:.3f}\",\n",
    "        fontsize=5,\n",
    "        color=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        [median_nu, median_nu],\n",
    "        [0.0001, 1000],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.7,\n",
    "        color=\"black\",\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 400, r\"median($\\nu$)\", fontsize=6, color=\"black\", alpha=0.9\n",
    "    )\n",
    "    plt.text(\n",
    "        median_nu + 0.005, 200, f\"{median_nu:.3f}\", fontsize=5, color=\"black\", alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Adjust axes & legend\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(0.0001, 1000)\n",
    "    plt.xlim(0, 0.305)\n",
    "    plt.legend(\n",
    "        new_handles,\n",
    "        new_labels,\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=2,\n",
    "        borderaxespad=0.0,\n",
    "        title=\"Mean EER per Owner\\n(Validation Results)\",\n",
    "        title_fontsize=5,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_session_probability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_session_probability(y_impostor, subject, session):\n",
    "    \"\"\"Plot the owner probability for every sample of session.\"\"\"\n",
    "    df_y = pd.DataFrame(y_impostor).transpose()\n",
    "    import matplotlib.ticker as ticker\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig = plt.figure(figsize=(5.473, 0.6), dpi=180)\n",
    "    heatmap = sns.heatmap(\n",
    "        df_y,\n",
    "        cmap=\"RdYlGn_r\",\n",
    "        annot=False,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={\"aspect\": 10, \"pad\": 0.01, \"shrink\": 1},\n",
    "        yticklabels=0,\n",
    "    )\n",
    "    start, end = heatmap.get_xlim()\n",
    "    stepsize = 50\n",
    "    heatmap.xaxis.set_ticks(np.arange(start, end, stepsize))\n",
    "    heatmap.xaxis.set_major_formatter(ticker.FormatStrFormatter(\"%0.f\"))\n",
    "\n",
    "    # Adjust labels\n",
    "    heatmap.set_ylabel(\n",
    "        f'Impostor Propability\\n(Owner \"{subject}\")',\n",
    "        labelpad=5,\n",
    "        rotation=0,\n",
    "        ha=\"right\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "    heatmap.set_xlabel(f'Samples of Session \"{session}\"', labelpad=3)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    y_impostor = [0.2, 0.3, 0.15, 0.4, 0.7, 0.65, 0.1, 0.3, 0.9, 0.01, 0.87, 0.79, 0.93, 0.98]\n",
    "    utils_plot_session_probability(y_impostor, \"123456\", \"7890\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_training_loss(history):\n",
    "    \"\"\"Plot Train/Valid Loss during Epochs.\"\"\"\n",
    "    fig = plt.figure(figsize=(5.473, 2.7), dpi=180)\n",
    "    plt.plot(history.history[\"loss\"], label=\"train\", color=\"tab:blue\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"valid\", color=MAGENTA)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    HistoryDummy = type(\"History\", (object,), {})\n",
    "    history = HistoryDummy()\n",
    "    history.history = {}\n",
    "    history.history[\"loss\"] = [0.6, 0.4, 0.3, 0.2, 0.21, 0.15]\n",
    "    history.history[\"val_loss\"] = [0.9, 0.7, 0.5, 0.4, 0.35, 0.3]\n",
    "    utils_plot_training_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_acc_eer_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_acc_eer_dist(df_plot, y_col):\n",
    "    n_subject = len(df_plot[\"Owner\"].unique()) - 1\n",
    "    mean_col = df_plot[y_col].mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(5.473, 2), dpi=180)\n",
    "    ax = sns.boxplot(x=\"Owner\", y=y_col, data=df_plot, **utils_boxplot_style)\n",
    "    ax.set_ylim((0, 1))\n",
    "\n",
    "    plt.plot(\n",
    "        [-0.6, n_subject + 0.6],\n",
    "        [mean_col, mean_col],\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=1,\n",
    "        color=MAGENTA,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.text(n_subject + 0.6, mean_col, f\"mean\", fontsize=6, color=MAGENTA)\n",
    "    plt.text(\n",
    "        n_subject + 0.6, mean_col - 0.04, f\"{mean_col:.3f}\", fontsize=4.5, color=MAGENTA\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(f\"Overall mean: {mean_col:.4f}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_training_delay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_training_delay(df_plot):\n",
    "    fig = plt.figure(figsize=(5.473/2.05, 1.7), dpi=180)\n",
    "    ax = sns.lineplot(x=\"Training Data in Seconds\", y=\"Test EER\", data=df_plot, **utils_lineplot_style)\n",
    "    ax.set_ylim((0, 0.6))\n",
    "    ax.get_children()[1].set_marker(\"o\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_distance_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_distance_hist(dist_positive, dist_negative, thres, desc, margin=None):\n",
    "    \"\"\"Plot histogramm of Euclidean Distances for Positive & Negative Pairs.\"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Plot Distributions\n",
    "    plt.figure(figsize=(5.473, 0.6), dpi=180)\n",
    "    bins = np.linspace(\n",
    "        min(dist_positive.min(), dist_negative.min()),\n",
    "        max(dist_positive.max(), dist_negative.max()),\n",
    "        num=21,\n",
    "    )\n",
    "    g1 = sns.distplot(\n",
    "        dist_positive,\n",
    "        label=\"positive pairs\",\n",
    "        bins=bins,\n",
    "        axlabel=False,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    g2 = sns.distplot(\n",
    "        dist_negative,\n",
    "        label=\"negative pairs\",\n",
    "        bins=bins,\n",
    "        hist_kws=dict(edgecolor=\"k\", lw=0.5),\n",
    "        kde_kws=dict(linewidth=0.8),\n",
    "        color=\"tab:gray\",\n",
    "    )\n",
    "\n",
    "    # Plot vertical lines\n",
    "    if thres > 0:\n",
    "        max_y = max(g1.get_ylim()[1], g2.get_ylim()[1])\n",
    "        plt.axvline(x=thres, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "        plt.text(\n",
    "            x=thres + 0.001,\n",
    "            y=max_y * 0.65,\n",
    "            s=f\"EER Threshold\\n({thres:.2f})\",\n",
    "            color=MAGENTA,\n",
    "            weight=\"bold\",\n",
    "            fontsize=5,\n",
    "            alpha=1\n",
    "        )\n",
    "        if margin:\n",
    "            plt.axvline(x=margin, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7)\n",
    "            plt.text(\n",
    "                x=margin + 0.001,\n",
    "                y=max_y * 0.15,\n",
    "                s=f\"Margin\\n({margin})\",\n",
    "                color=MAGENTA,\n",
    "                weight=\"bold\",\n",
    "                fontsize=5,\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(\n",
    "        loc=\"upper right\",\n",
    "        title=f\"{desc} Distances\",\n",
    "        title_fontsize=5,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    dist_pos = np.array([0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.5, 0.8])\n",
    "    dist_neg = np.array([0.4, 0.5, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.8, 1, 1])\n",
    "    utils_plot_distance_hist(\n",
    "        dist_pos, dist_neg, thres=0.4, desc=\"Pair\", fig_size=(12, 4), margin=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_plot_detect_delay():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_plot_detect_delay(df_plot, factor, xlim):\n",
    "    # Desired Error Margin\n",
    "    THRES_ERROR_MARGIN = 0.025  # +-\n",
    "\n",
    "    # Start Plotting\n",
    "    fig, axes = plt.subplots(\n",
    "        ncols=5, nrows=2, sharex=True, sharey=True, figsize=(5.473, 3), dpi=180\n",
    "    )\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.ylim(0, 0.55)\n",
    "\n",
    "    col = 0\n",
    "    row = 0\n",
    "    n_samples_all = []\n",
    "    n_seconds_all = []\n",
    "    \n",
    "    for owner, df_group in df_plot[[\"owner\", \"test_eer\"]].groupby(\"owner\"):\n",
    "        # Calc expanding statistics\n",
    "        df_temp = df_group.expanding().mean().reset_index(drop=True)[[\"test_eer\"]]\n",
    "\n",
    "        std = df_temp[\"test_eer\"].std()\n",
    "        n_samples = (\n",
    "            math.ceil((1.96 * std) / THRES_ERROR_MARGIN) ** 2\n",
    "        )  # 1.96 z-score for 95% confidence,confidence with = 2*0.05 EER\n",
    "        n_seconds = n_samples * factor\n",
    "        \n",
    "        n_samples_all.append(n_samples)\n",
    "        n_seconds_all.append(n_seconds)\n",
    "        \n",
    "        df_temp[\"Owner\"] = owner\n",
    "        df_temp = df_temp.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        df_temp[\"Expanding Mean\"] = df_temp[\"test_eer\"].expanding().mean()\n",
    "        df_temp[\"Lower Confidence\"] = (\n",
    "            df_temp[\"test_eer\"]\n",
    "            .expanding()\n",
    "            .apply(lambda x: sms.DescrStatsW(x).tconfint_mean()[0], raw=False)\n",
    "        )\n",
    "        df_temp[\"Upper Confidence\"] = (\n",
    "            df_temp[\"test_eer\"]\n",
    "            .expanding()\n",
    "            .apply(lambda x: sms.DescrStatsW(x).tconfint_mean()[1], raw=False)\n",
    "        )\n",
    "        df_temp = (\n",
    "            df_temp.reset_index()\n",
    "            .drop(columns=\"test_eer\")\n",
    "            .rename(columns={\"index\": \"Samples\"})\n",
    "        )\n",
    "        df_temp[\"Seconds\"] = (df_temp[\"Samples\"] + 1) * factor\n",
    "\n",
    "        \n",
    "        # Plot, but only if not already plotted 10 owners\n",
    "        if col * row >= 10:\n",
    "            continue\n",
    "            \n",
    "        axes[row][col].plot(df_temp[\"Seconds\"], df_temp[\"Expanding Mean\"], lw=0.5)\n",
    "        axes[row][col].set_title(f\"Owner {owner}\", pad=3, fontsize=5)\n",
    "        axes[row][col].fill_between(\n",
    "            df_temp[\"Seconds\"],\n",
    "            df_temp[\"Upper Confidence\"],\n",
    "            df_temp[\"Lower Confidence\"],\n",
    "            color=\"tab:blue\",\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        axes[row][col].axvline(\n",
    "            x=n_seconds, color=MAGENTA, linestyle=\"--\", lw=0.8, alpha=0.7\n",
    "        )\n",
    "        axes[row][col].text(\n",
    "            x=n_seconds + 6,\n",
    "            y=0.38,\n",
    "            s=f\"{n_seconds} s\",\n",
    "            color=MAGENTA,\n",
    "            fontsize=6,\n",
    "            rotation=90,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "        axes[row][col].text(\n",
    "            x=n_seconds + 25,\n",
    "            y=0.38,\n",
    "            s=f\"{n_samples} samples\",            \n",
    "            color=MAGENTA,\n",
    "            fontsize=4.4,\n",
    "            rotation=90,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "        col += 1\n",
    "        if col == 5:\n",
    "            col = 0\n",
    "            row += 1\n",
    "        if row >= 2:\n",
    "            break\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    fig.text(0.5, 0.0075, \"Seconds of Testing Data\", ha=\"center\", va=\"center\", fontsize=6)\n",
    "    fig.text(\n",
    "        0.008,\n",
    "        0.5,\n",
    "        \"Expanding mean EER\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=6,\n",
    "    )    \n",
    "    print(f\"Mean samples: {np.mean(n_samples_all):.1f}\")\n",
    "    print(f\"Mean seconds: {np.mean(n_seconds_all):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_animate_plot &lt;class&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils_animate_plot:\n",
    "    \"\"\"Helper Class to generate an animated plot showing owner probability over session.\"\"\"\n",
    "    def __init__(self, y_true, y_pred, dfs=None, freq=0.33, speed=1, sga_auth_seconds=10, title=\"\"):\n",
    "        self.y_true = pd.Series(np.where(y_true > 0, 0, 1)) # Invert, because we are showing impostor probability\n",
    "        self.y_pred = y_pred\n",
    "        self.current_color = None\n",
    "        self.dfs = dfs\n",
    "        self.interval = (1000 / freq) / speed\n",
    "        self.title = title\n",
    "        self.speed = speed\n",
    "        self.freq = freq\n",
    "        self.sga_auth_seconds = sga_auth_seconds\n",
    "        self.sga_auth_samples = int(sga_auth_seconds // freq)\n",
    "        self.y_len = len(y_pred)\n",
    "\n",
    "    def setup_canvas(self):\n",
    "        sensor_colors = [\"tab:cyan\", \"tab:pink\", \"tab:olive\"]\n",
    "\n",
    "        # Setup Canvas\n",
    "        self.fig, ax = plt.subplots(len(self.dfs) + 1, 1, sharex=\"col\", figsize=(16, 9))\n",
    "\n",
    "        # Auth Probability\n",
    "        ax[0].set_xlim((0, self.y_len // self.freq))\n",
    "        ax[0].set_ylim((-0.1, 1.1))\n",
    "        self.line_y, = ax[0].plot([], [], lw=2, color=\"gray\", alpha=0.5)\n",
    "        self.line_ground_truth, = ax[0].plot([], [], lw=1, color=\"gray\", alpha=0.5, linestyle=\":\")        \n",
    "        self.line_green, = ax[0].plot([], [], lw=2, color=\"tab:green\")\n",
    "        self.line_yellow, = ax[0].plot([], [], lw=2, color=\"tab:orange\")\n",
    "        self.line_red, = ax[0].plot([], [], lw=2, color=\"tab:red\")\n",
    "        ax[0].set_title(self.title, pad=14)\n",
    "        ax[0].legend(\n",
    "            (self.line_y, self.line_green, self.line_ground_truth),\n",
    "            (r\"$\\hat{\\gamma}$ Impostor\", r\"$\\hat{\\gamma}_{SMA}$\", r\"Ground Truth\"),\n",
    "            loc=4,\n",
    "            fontsize=\"small\",\n",
    "            fancybox=True,\n",
    "        )\n",
    "\n",
    "        self.sensors = []\n",
    "        for idx, df in enumerate(self.dfs):\n",
    "            ax[idx + 1].set_xlim((0, self.y_len // self.freq))\n",
    "            ax[idx + 1].set_ylim((df.max().max() * 1.1, df.min().min() * 1.1))\n",
    "            sensor = {}\n",
    "            for idx2, value in enumerate(df.columns):\n",
    "                sensor[value], = ax[idx + 1].plot(\n",
    "                    [], [], lw=2, alpha=0.5, color=sensor_colors[idx2], label=value\n",
    "                )\n",
    "            ax[idx + 1].legend(loc=4, fontsize=\"small\", fancybox=True)\n",
    "            if idx == len(self.dfs) - 1:\n",
    "                ax[idx + 1].set_xlabel(f\"Time in seconds\\n(playing {self.speed}x)\")\n",
    "            self.sensors.append(sensor)\n",
    "\n",
    "        self.y_colors = {\"green\": [], \"yellow\": [], \"red\": []}\n",
    "\n",
    "        # Create animated video\n",
    "        anim = animation.FuncAnimation(\n",
    "            self.fig,\n",
    "            self.animate,\n",
    "            frames=self.y_len,\n",
    "            interval=self.interval,\n",
    "            blit=True,\n",
    "            init_func=self.init_data,\n",
    "            repeat=False,\n",
    "        )\n",
    "\n",
    "        # Prevent from showing static canvas\n",
    "        plt.close(self.fig)\n",
    "\n",
    "        return anim\n",
    "\n",
    "    def init_data(self):\n",
    "        # (Predicted) auth data\n",
    "        self.line_green.set_data([], [])\n",
    "        self.line_yellow.set_data([], [])\n",
    "        self.line_red.set_data([], [])\n",
    "        self.line_y.set_data([], [])\n",
    "        self.line_ground_truth.set_data([], [])\n",
    "        \n",
    "        # Sensor data\n",
    "        for sensor in self.sensors:\n",
    "            for key in sensor:\n",
    "                sensor[key].set_data([], [])\n",
    "\n",
    "        return (self.line_green,)\n",
    "\n",
    "    def next(self, i):\n",
    "        j = max((i - self.sga_auth_samples), 0)\n",
    "        auth = self.y_pred.iloc[j:i].mean()  # #FIXME: Int is strange here\n",
    "\n",
    "        # Which level do we have?\n",
    "        if auth < 0.3:\n",
    "            next_color = \"green\"\n",
    "        elif auth < 0.5:\n",
    "            next_color = \"yellow\"\n",
    "        else:\n",
    "            next_color = \"red\"\n",
    "\n",
    "        # Add to respective line\n",
    "        if next_color == self.current_color:\n",
    "            for key in self.y_colors:\n",
    "                if key == next_color:\n",
    "                    self.y_colors[key].append(auth)\n",
    "                else:\n",
    "                    self.y_colors[key].append(None)\n",
    "        else:\n",
    "            for key in self.y_colors:\n",
    "                if key == next_color or key == self.current_color:\n",
    "                    self.y_colors[key].append(auth)\n",
    "                else:\n",
    "                    self.y_colors[key].append(None)\n",
    "\n",
    "        self.current_color = next_color\n",
    "\n",
    "    def animate(self, i):\n",
    "        self.next(i)\n",
    "        x = list(range(0, i + 1))\n",
    "        x = [v / self.freq for v in x]\n",
    "\n",
    "        # Animate auth data\n",
    "        self.line_green.set_data(x, self.y_colors[\"green\"])\n",
    "        self.line_yellow.set_data(x, self.y_colors[\"yellow\"])\n",
    "        self.line_red.set_data(x, self.y_colors[\"red\"])\n",
    "        self.line_y.set_data(x, self.y_pred.iloc[: i + 1])\n",
    "        self.line_ground_truth.set_data(x, self.y_true.iloc[: i + 1])\n",
    "\n",
    "        # Animate sensor data\n",
    "        for idx, sensor in enumerate(self.sensors):\n",
    "            for key in sensor:\n",
    "                self.sensors[idx][key].set_data(\n",
    "                    x, self.dfs[idx][key].iloc[: i + 1].values\n",
    "                )\n",
    "\n",
    "        return (self.line_green,)\n",
    "\n",
    "    def show(self):\n",
    "        anim = self.setup_canvas()\n",
    "        return HTML(anim.to_html5_video())\n",
    "\n",
    "    def save(self, file_name):\n",
    "        anim = self.setup_canvas()\n",
    "        Writer = animation.writers[\"ffmpeg\"]\n",
    "        writer = Writer(fps=1000 / self.interval, bitrate=2400)\n",
    "        anim.save(file_name, writer=writer, dpi=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_simulate_auth_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_simulate_auth_session(clf, dl, seed=712, filepath=None):\n",
    "    \n",
    "    DESCRIPTIVE_COLS = [\n",
    "        \"subject\",\n",
    "        \"session\",\n",
    "        \"sys_time\",\n",
    "        \"min_sys_time\",\n",
    "        \"max_sys_time\",\n",
    "        \"label\",\n",
    "        \"gesture_scenario\",\n",
    "    ]  # columns to be removed from features, as they indicate target class\n",
    "\n",
    "    print(f\"Training Model...\")\n",
    "    \n",
    "    dl.split_train_valid_test(seed=seed)\n",
    "\n",
    "    df_train = dl.dfs[\"train_t\"]\n",
    "    df_test = dl.dfs[\"test\"]\n",
    "    owner = dl.test_owner\n",
    "\n",
    "    owner_sessions = list(df_test[df_test[\"label\"] == 1][\"session\"].unique())\n",
    "    impostor_sessions = list(df_test[df_test[\"label\"] == -1][\"session\"].unique())\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random_owner_sessions = random.sample(owner_sessions, 2)\n",
    "    random_impostor_sesions = random.sample(impostor_sessions, 2)\n",
    "\n",
    "    df_owner = df_test[df_test[\"session\"] == random_owner_sessions[0]].copy()\n",
    "    df_impostor = df_test[df_test[\"session\"] == random_impostor_sesions[0]].copy()\n",
    "    df_owner_m = df_test[df_test[\"session\"] == random_owner_sessions[1]].copy()\n",
    "    df_impostor_m = df_test[df_test[\"session\"] == random_impostor_sesions[1]].copy()\n",
    "    df_mixed = pd.concat(\n",
    "        [\n",
    "            df_owner_m.iloc[: int(len(df_owner_m) // 2)],\n",
    "            df_impostor_m.iloc[int(len(df_owner_m) // 2) :],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dfs = [\n",
    "        {\n",
    "            \"set\": \"Owner\",\n",
    "            \"df\": df_owner,\n",
    "            \"owner\": owner,\n",
    "            \"session\": random_owner_sessions[0],\n",
    "        },\n",
    "        {\n",
    "            \"set\": \"Impostor\",\n",
    "            \"df\": df_impostor,\n",
    "            \"owner\": owner,\n",
    "            \"session\": random_impostor_sesions[0],\n",
    "        },\n",
    "        {\n",
    "            \"set\": \"Mixed\",\n",
    "            \"df\": df_mixed,\n",
    "            \"owner\": owner,\n",
    "            \"session\": random_owner_sessions[1] + \" & \" + random_impostor_sesions[1],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    y_train = df_train[\"label\"].values\n",
    "    X_train = df_train.drop(columns=DESCRIPTIVE_COLS, errors=\"ignore\").values\n",
    "    _ = clf.fit(X_train, y_train)\n",
    "\n",
    "    for idx, sim_set in enumerate(dfs):\n",
    "        df = sim_set[\"df\"]\n",
    "\n",
    "        y_test = df[\"label\"].values\n",
    "        X_test = df.drop(columns=DESCRIPTIVE_COLS, errors=\"ignore\").values\n",
    "\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_pred = clf.predict_proba(X_test)  \n",
    "            y_pred = y_pred[:,0]  # We only need impostor probability\n",
    "        else:  # Some classifiers like ocsvm can't predict probability\n",
    "            y_pred = clf.predict(X_test)      \n",
    "            y_pred = np.where(y_pred == -1, 1, 0)\n",
    "        df[\"proba\"] = y_pred\n",
    "            \n",
    "        a = animate_plot(\n",
    "            df[\"label\"],\n",
    "            df[\"proba\"],\n",
    "            dfs=[\n",
    "                df[[\"mean_1d_gyr_x\", \"mean_1d_gyr_y\", \"mean_1d_gyr_z\"]],\n",
    "                df[[\"mean_1d_acc_x\", \"mean_1d_acc_y\", \"mean_1d_acc_z\"]],\n",
    "                df[[\"mean_1d_mag_x\", \"mean_1d_mag_y\", \"mean_1d_mag_z\"]],\n",
    "            ],\n",
    "            freq=0.5,\n",
    "            speed=20,\n",
    "            title=f\"Owner: {sim_set['owner']} - Session: {sim_set['session']}\",\n",
    "        )\n",
    "        print(f\"Create animated plot...\")        \n",
    "        display(a.show())     \n",
    "        print(f\"Saving...\")        \n",
    "        if filepath:\n",
    "            a.save(str((filepath / f\"simulated_auth_{idx}.mpg\").resolve()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 3. Reporting/printing related <a id='3'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_ppp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_ppp(P):\n",
    "    \"\"\"Pretty print parameters of an experiment.\"\"\"\n",
    "    df = pd.DataFrame([asdict(P)])\n",
    "    df = df.T\n",
    "    df.columns = [\"Value\"]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    P_temp = {\n",
    "        \"Parameter 1\": \"Value 1\",\n",
    "        \"margin\": 0.5,\n",
    "        \"ocsvm_nu\": 0.3,\n",
    "        \"ocsvm_gamma\": 13,\n",
    "    }\n",
    "    utils_ppp(P_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_split_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_split_report(df):\n",
    "    \"\"\"Print basic info about a dataset.\"\"\"\n",
    "    print(f\"Unique subjects: {len(df['subject'].unique())}\")\n",
    "    print(f\"Unique sessions: {len(df['session'].unique())}\")\n",
    "    print(\"\\nHead:\")\n",
    "    display(df.head())\n",
    "    df_sessions = df[[\"subject\", \"session\", \"task_type\"]].groupby(\"session\").max()\n",
    "    df_sessions = pd.concat([df_sessions, pd.get_dummies(df_sessions[\"task_type\"])], axis=1)\n",
    "    df_subjects = df_sessions.groupby(\"subject\").sum().drop(columns=\"task_type\")\n",
    "    print(\"\\n\\nSessions' Task Types per subject:\")\n",
    "    display(df_subjects.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [\"x\", \"x\", \"x\", \"y\"],\n",
    "            \"session\": [\"a\", \"b\", \"c\", \"d\"],\n",
    "            \"task_type\": [1, 1, 2, 2],\n",
    "        }\n",
    "    )\n",
    "    utils_split_report(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_cv_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_cv_report(random_search, owner, impostors):\n",
    "    \"\"\"Transform the random_search.cv_results_ into nice formatted dataframe.\"\"\"\n",
    "    # Create report\n",
    "    df_report = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "    # Add owner information\n",
    "    df_report[\"owner\"] = owner\n",
    "\n",
    "    # Drop uninteressting columns\n",
    "    drop_columns = [col for col in df_report.columns if \"_train_\" in col]\n",
    "    drop_columns = drop_columns + [col for col in df_report.columns if col.startswith(\"split\") and (col.endswith(\"recall\") or col.endswith(\"precision\") or col.endswith(\"f1\") or col.endswith(\"roc_auc\"))]\n",
    "    drop_columns = drop_columns + [\"params\"]\n",
    "    df_report = df_report.drop(columns=drop_columns)\n",
    "\n",
    "    # Flip sign of eer (revert flip by sklearn scorer)\n",
    "    eer_columns = [col for col in df_report.columns if col.endswith(\"_eer\")]\n",
    "    df_report[eer_columns] = df_report[eer_columns].abs()\n",
    "    \n",
    "    # Rename split result columns with impostor-ids used in split\n",
    "    rename_cols = {}\n",
    "    for idx, impostor in enumerate(impostors):\n",
    "        to_rename_cols = [col for col in df_report.columns if col.startswith(f\"split{idx}\")]\n",
    "        for col in to_rename_cols:\n",
    "            rename_cols[col] = str(impostor)+col[len(f\"split{idx}\"):]\n",
    "    df_report = df_report.rename(columns=rename_cols)      \n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"Performing Dummy RandomSearch...\")\n",
    "    from sklearn import svm, datasets\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 2, 3, 4, 5, 6, 7, 10]}\n",
    "    svc = svm.SVC(gamma=\"scale\")\n",
    "    clf = RandomizedSearchCV(svc, parameters, cv=3, iid=False)\n",
    "    clf.fit(iris.data, iris.target)\n",
    "    print(\"Create report:\")\n",
    "    df_temp = utils_cv_report(clf, \"owner x\", [\"impo_1\", \"impo_2\", \"impo_3\"])\n",
    "    display(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 4. Scoring related <a id='4'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_eer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_eer(y_true, y_pred, return_threshold=False):\n",
    "    \"\"\"Calculate the Equal Error Rate.\n",
    "\n",
    "    Based on https://stackoverflow.com/a/49555212, https://yangcha.github.io/EER-ROC/\n",
    "    and https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "\n",
    "    Arguments:\n",
    "        y_true {np.array}  -- Actual labels\n",
    "        y_pred {np.array}  -- Predicted labels or probability\n",
    "        \n",
    "    Returns:\n",
    "        float              -- Equal Error Rate        \n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)  # Calculated threshold, not needed for score\n",
    "    if return_threshold:\n",
    "        return eer, thresh\n",
    "    else:\n",
    "        return eer\n",
    "    \n",
    "def utils_eer_minus_one(y_true, y_pred, return_threshold=False):\n",
    "    \"\"\"Calculate the Equal Error Rate.\n",
    "\n",
    "    Based on https://stackoverflow.com/a/49555212, https://yangcha.github.io/EER-ROC/\n",
    "    and https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "\n",
    "    Arguments:\n",
    "        y_true {np.array}  -- Actual labels\n",
    "        y_pred {np.array}  -- Predicted labels or probability\n",
    "        \n",
    "    Returns:\n",
    "        float              -- Equal Error Rate        \n",
    "    \"\"\"\n",
    "    y_pred = y_pred * -1\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    eer = brentq(lambda x: 1.0 - x - interp1d(fpr, tpr)(x), 0.0, 1.0)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)  # Calculated threshold, not needed for score\n",
    "    if return_threshold:\n",
    "        return eer, thresh\n",
    "    else:\n",
    "        return eer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1], [0, 0.9, 0.1, 0.74, 0.8], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Arbitrary case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [-1, -1, -1, 1, 1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Best case\")\n",
    "\n",
    "    temp_eer, tres = utils_eer(\n",
    "        [1, 1, 1, -1, -1], [0.1, 0.2, 0.3, 1, 0.9], return_threshold=True\n",
    "    )\n",
    "    print(f\"EER: {temp_eer:.3f}, Threshold: {tres:.3f} <-- Worse case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_eer_scorer &lt;sklearn scorer&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3693c47f78cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils_eer_scorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils_eer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreater_is_better\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mutils_eer_scorer_REV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils_eer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreater_is_better\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "utils_eer_scorer = make_scorer(utils_eer, greater_is_better=False)\n",
    "utils_eer_scorer_minus_one = make_scorer(utils_eer_minus_one, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_accuracy_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_accuracy_proba(y_true, y_score):\n",
    "    \"\"\"Calculates accuracy score, but takes class probability as input and uses threshold of EER.\n",
    "\n",
    "    Arguments:\n",
    "        y_true {np.array}  -- Actual labels\n",
    "        y_score {np.array}  -- Predicted probability\n",
    "        \n",
    "    Returns:\n",
    "        float              -- F1 Score         \n",
    "    \"\"\"\n",
    "    _, thres = utils_eer(y_true, y_score, return_threshold=True)\n",
    "    y_pred = np.where(y_score >= thres, 1, -1)\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_accuracy_proba_scorer &lt;sklearn scorer&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_accuracy_proba_scorer = make_scorer(utils_accuracy_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    temp_acc = utils_accuracy_proba([-1, -1, -1, 1, 1], [0, 0.9, 0.1, 0.74, 0.8])\n",
    "    print(f\"ACC: {temp_acc:.3f} <-- Arbitrary case\")\n",
    "\n",
    "    temp_acc = utils_accuracy_proba([-1, -1, -1, 1, 1], [0.1, 0.2, 0.3, 1, 0.9])\n",
    "    print(f\"ACC: {temp_acc:.3f} <-- Best case\")\n",
    "\n",
    "    temp_acc = utils_accuracy_proba([1, 1, 1, 0, 0], [0.1, 0.2, 0.3, 1, 0.9])\n",
    "    print(f\"ACC: {temp_acc:.3f} <-- Worse case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 5. Data Preparation related <a id='5'>&nbsp;</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_get_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_get_scaler(scaler_name):\n",
    "    \"\"\"Returns scaler of given type.\"\"\"\n",
    "    if scaler_name == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_name == \"minmax_fix\":\n",
    "        scaler = MinMaxScaler()      \n",
    "    elif scaler_name == \"std\":\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_name == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler_name == \"robust_no_center\":\n",
    "        scaler = RobustScaler(with_centering=False)        \n",
    "    else:\n",
    "        raise BaseException(\"Error: Not a valid scaler: minmax, std, robust.\")\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    temp_scaler = utils_get_scaler(\"minmax\")\n",
    "    print(temp_scaler)\n",
    "    temp_scaler = utils_get_scaler(\"std\")\n",
    "    print(temp_scaler)\n",
    "    temp_scaler = utils_get_scaler(\"robust\")\n",
    "    print(temp_scaler)\n",
    "    temp_scaler = utils_get_scaler(\"minmax_fix\")\n",
    "    print(temp_scaler)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_custom_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_custom_scale(\n",
    "    df, scale_cols, feature_cols=None, scope=\"subject\", scaler_name=\"minmax\", plot=False\n",
    "):\n",
    "    def get_dummy_data(feature_cols, factor=None):\n",
    "        dummy_min = []\n",
    "        dummy_max = []\n",
    "        for c in feature_cols:\n",
    "            if c.startswith(\"acc\"):\n",
    "                dummy_min.append(-5)\n",
    "                dummy_max.append(15)\n",
    "            elif c.startswith(\"gyr\"):\n",
    "                dummy_min.append(-5)\n",
    "                dummy_max.append(5)\n",
    "            elif c.startswith(\"mag\"):\n",
    "                dummy_min.append(-75)\n",
    "                dummy_max.append(75)\n",
    "        if factor:\n",
    "            dummy_min = dummy_min * int(factor / len(feature_cols))\n",
    "            dummy_max = dummy_max * int(factor / len(feature_cols))\n",
    "        return np.array([dummy_min, dummy_max])\n",
    "\n",
    "    # Scalers for session/subject\n",
    "    scalers = {}\n",
    "\n",
    "    if plot:\n",
    "        print(\"Before Scaling:\")\n",
    "        sess = df[\"session\"].unique()[0]\n",
    "        fig = plt.figure(figsize=(5.473, 2), dpi=180)\n",
    "        df[df[\"session\"] == sess][scale_cols].iloc[:3000].plot(ax=plt.gca())\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "    dfs = []\n",
    "    if scope in [\"session\", \"subject\"]:\n",
    "        for group_name, df_group in tqdm(\n",
    "            df.groupby(scope, sort=False), desc=f\"{scope}s\", leave=False\n",
    "        ):\n",
    "            df_temp = df_group.copy()\n",
    "            scaler = utils_get_scaler(scaler_name)\n",
    "\n",
    "            if len(scale_cols) > 1:\n",
    "                # Features come in different columns\n",
    "                if scaler_name == \"minmax_fix\":\n",
    "                    scaler.fit(get_dummy_data(feature_cols))\n",
    "                    df_temp[scale_cols] = scaler.transform(df_temp[scale_cols].values)\n",
    "                    # Clip outliers\n",
    "                    df_temp[scale_cols] = df_temp[scale_cols].clip(lower=0, upper=1)\n",
    "                else:\n",
    "                    # Feature come in multiple columns\n",
    "                    df_temp[scale_cols] = scaler.fit_transform(\n",
    "                        df_temp[scale_cols].values\n",
    "                    )\n",
    "            else:\n",
    "                # Features come in single column (X), as nested array\n",
    "                dims_X = np.stack(df_temp[scale_cols[0]].apply(np.asarray).values).shape\n",
    "                ary_temp = np.vstack(df_temp[scale_cols[0]].apply(np.asarray).values)\n",
    "\n",
    "                if scaler_name == \"minmax_fix\":\n",
    "                    scaler.fit(get_dummy_data(feature_cols, factor=ary_temp.shape[1]))\n",
    "                    ary_temp = scaler.transform(ary_temp)\n",
    "                    # Clip outliers\n",
    "                    ary_temp = np.clip(ary_temp, a_min=0, a_max=1)\n",
    "                else:\n",
    "                    ary_temp = scaler.fit_transform(ary_temp)\n",
    "\n",
    "                ary_temp = ary_temp.reshape(dims_X)\n",
    "                df_temp[scale_cols[0]] = ary_temp.tolist()\n",
    "\n",
    "            scalers[group_name] = scaler\n",
    "            dfs.append(df_temp)\n",
    "    elif scope == \"global\":\n",
    "        scaler = utils_get_scaler(scaler_name)\n",
    "        df[scale_cols] = scaler.fit_transform(df[scale_cols].values)\n",
    "        scalers = scaler\n",
    "    else:\n",
    "        print(\"No valid scaling method provided. (session, subject, all, fixed)\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "\n",
    "    if plot:\n",
    "        print(\"After Scaling:\")\n",
    "        fig = plt.figure(figsize=(5.473, 2), dpi=180)\n",
    "        df[df[\"session\"] == sess][scale_cols].iloc[:3000].plot(ax=plt.gca())\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "    return df, scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"TESTING SINGLE COLUMN FEATURES WITH MINMAX SCALER:\")\n",
    "    df_before = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [\"a\"],\n",
    "            \"session\": [1],\n",
    "            \"X\": [[[0, 0, 0], [0.3, 4, 50], [1, 10, 100]]],\n",
    "        }\n",
    "    )\n",
    "    print(\"before:\")\n",
    "    display(df_before)\n",
    "    df_after, scaler_temp = utils_custom_scale(\n",
    "        df_before, [\"X\"], scope=\"subject\", scaler_name=\"minmax\", plot=False\n",
    "    )\n",
    "    print(\"after:\")\n",
    "    display(df_after)\n",
    "    print(\"scalers:\")\n",
    "    print(scaler_temp)\n",
    "\n",
    "    print(\"\\n\\nTESTING MULTI COLUMN FEATURES WITH STANDARDSCALER:\")\n",
    "    df_before = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\"],\n",
    "            \"session\": [1, 1, 1, 1, 2, 2, 2, 2],\n",
    "            \"acc_X\": [0, 100, 30, 45, 0, 10, 5, 6],\n",
    "            \"acc_Y\": [15, 5, 25, 20, 0, 10, 5, 2],\n",
    "        }\n",
    "    )\n",
    "    df_after, scaler_temp = utils_custom_scale(\n",
    "        df_before, [\"acc_X\", \"acc_Y\"], scope=\"subject\", scaler_name=\"minmax_fix\", plot=True, feature_cols=[\"acc_X\", \"acc_Y\"]\n",
    "    )\n",
    "    print(\"before:\")\n",
    "    display(df_before)\n",
    "    print(\"after:\")\n",
    "    display(df_after)\n",
    "    print(\"scalers:\")\n",
    "    print(scaler_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_scale_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_scale_X(df, scaler):\n",
    "    df = df.copy()\n",
    "    dims_X = np.stack(df[\"X\"].apply(np.asarray).values).shape\n",
    "    ary_temp = np.vstack(df[\"X\"].apply(np.asarray).values)\n",
    "    ary_temp = scaler.transform(ary_temp)\n",
    "    ary_temp = ary_temp.reshape(dims_X)\n",
    "    df[\"X\"] = ary_temp.tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    df_initial = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [\"a\", \"b\"],\n",
    "            \"session\": [1,  2],\n",
    "            \"X\": [[[0, 0, 0,], [1, 10, 100]], [[0, 0, 0,], [0.5, 5, 50]]],\n",
    "        }\n",
    "    )\n",
    "    print(\"Fit scalers for subjects a and b on:\")\n",
    "    display(df_initial)\n",
    "    \n",
    "    df_initial, scaler_temp = utils_custom_scale(\n",
    "        df_initial, [\"X\"], scope=\"subject\", scaler_name=\"minmax\", plot=False\n",
    "    )\n",
    "    print(\"Scalers:\")\n",
    "    print(scaler_temp)    \n",
    "        \n",
    "    df_before = pd.DataFrame(\n",
    "        {\"Subject\": [\"a\"], \"session\": [3], \"X\": [[5, 5, 5]]}\n",
    "    )\n",
    "    print(\"Before normalization:\")\n",
    "    display(df_before)\n",
    "    \n",
    "    df_after_a = utils_scale_X(df_before, scaler_temp[\"a\"])\n",
    "    df_after_b = utils_scale_X(df_before, scaler_temp[\"b\"])\n",
    "    print(\"After normalization with scaler a:\")\n",
    "    display(df_after_a)\n",
    "    print(\"After normalization with scaler b:\")\n",
    "    display(df_after_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_gen_windows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_gen_windows(data_length, window_size, step_width):\n",
    "    \"Generate indices for window-slizes with given length & step width.\"\n",
    "    start = 0\n",
    "    while start < data_length:\n",
    "        yield start, start + window_size\n",
    "        start += step_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    g_temp = utils_gen_windows(10, 4, 3)\n",
    "    for idx in g_temp:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_reshape_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_reshape_features(df, feature_cols, window_size, step_width):\n",
    "    df_reshaped = pd.DataFrame()\n",
    "\n",
    "    for session, df_group in tqdm(df.groupby(\"session\"), desc=\"Session\", leave=False):\n",
    "        # Get subject & task type of session\n",
    "        subject = df_group[\"subject\"].unique()[0]\n",
    "        task_type = int(df_group[\"task_type\"].mode())\n",
    "\n",
    "        # Extract features\n",
    "        features = df_group[feature_cols].values\n",
    "\n",
    "        # Reshape features\n",
    "        new_feat_count = ((features.shape[0] - window_size) // step_width) + 1\n",
    "        reshaped_feat = np.empty((new_feat_count, window_size, len(feature_cols)))\n",
    "        for idx, window in enumerate(\n",
    "            utils_gen_windows(features.shape[0], window_size, step_width)\n",
    "        ):\n",
    "            new_row = features[window[0] : window[1]]\n",
    "            if idx < new_feat_count:\n",
    "                reshaped_feat[idx, :] = new_row\n",
    "\n",
    "        # Prepare new dataframe for current session\n",
    "        df_session = pd.DataFrame()\n",
    "        df_session[\"X\"] = reshaped_feat.tolist()\n",
    "        df_session[\"subject\"] = subject\n",
    "        df_session[\"session\"] = session\n",
    "        df_session[\"task_type\"] = task_type\n",
    "\n",
    "        # Concat session dataframe to overall\n",
    "        df_reshaped = df_reshaped.append(df_session, ignore_index=True)\n",
    "\n",
    "    return df_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [\"x\", \"x\", \"x\", \"x\", \"x\", \"y\", \"y\", \"y\", \"y\", \"y\"],\n",
    "            \"session\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n",
    "            \"task_type\": [1, 3, 1, 1, 1, 2, 2, 2, 2, 2],\n",
    "            \"X\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],\n",
    "        }\n",
    "    )\n",
    "    print(\"Before reshaping:\")\n",
    "    display(df_temp)\n",
    "\n",
    "    df_temp = utils_reshape_features(df_temp, [\"X\"], 3, 2)\n",
    "    print(\"After reshaping:\")\n",
    "    display(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### utils_generate_deep_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_generate_deep_features(df, model, variant):\n",
    "    # Predict deep features\n",
    "    X = np.stack(list(df[\"X\"].values))\n",
    "    \n",
    "    # 2D Filter Model needs flat 4th dimension\n",
    "    if variant == \"2d\":\n",
    "        X = X.reshape((*X.shape, 1))\n",
    "    \n",
    "    X_pred = model.predict([X, X])\n",
    "\n",
    "    # Overwrite original features\n",
    "    df[\"X\"] = [list(vect) for vect in X_pred]\n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### utils_generate_cv_scenarios()\n",
    "\n",
    "- We have $s$ subjects in our dataset, decided to use $n$ samples per session for training and $m$ samples per session for testing.\n",
    "- This results in $s - 1$ different **CV splits**: 1 owner is tested along with each of the $s - 1$ remaining subjects.\n",
    "- If the **training set** contains $18$ session from the owner with $n$ samples each, this leads to $18 \\cdot n$ training samples.\n",
    "- If the **validation set** contains $6$ session from the owner and $6$ sessions from a single impostor, with $m$ samples each, this leads to $(6+6) \\cdot m$ training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_generate_cv_scenarios(\n",
    "    df,\n",
    "    seed,    \n",
    "    samples_per_subject_train,\n",
    "    samples_per_subject_test,\n",
    "    limit_train_samples=None,\n",
    "    limit_test_samples=None,\n",
    "    scaler=None,\n",
    "    scaler_global=None,\n",
    "    scaler_scope=None,\n",
    "    deep_model=None,\n",
    "    model_variant=None,\n",
    "    feature_cols=None,\n",
    "):\n",
    "    \"\"\"Generate pairs of owner training data with owner/impostor validation data, along with CV mask.\"\"\"\n",
    "    # Split into training and validation set\n",
    "    df_train, df_valid_original = hmog.split_sessions(\n",
    "        df, sessions_per_type=1, seed=seed\n",
    "    )\n",
    "\n",
    "    # Use every subject once as owner and create training & valid data\n",
    "    subjects = list(df_train[\"subject\"].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(subjects)\n",
    "    for owner in subjects:\n",
    "        # Calc no of samples per session\n",
    "        n_sessions = len(df_train.query(\"subject == @owner\")[\"session\"].unique())\n",
    "        samples_per_session_train = math.ceil(samples_per_subject_train / n_sessions)\n",
    "        samples_per_session_test = math.ceil(samples_per_subject_test / n_sessions)\n",
    "        \n",
    "        # PREPARE TRAINING DATA\n",
    "        # ============================\n",
    "       \n",
    "        # Select random owner samples\n",
    "        df_owner_train = (\n",
    "            df_train.query(\"subject == @owner\")\n",
    "            .groupby(\"session\", group_keys=False)\n",
    "            .apply(\n",
    "                lambda x: x.sample(\n",
    "                    n=min(len(x), samples_per_session_train), random_state=seed\n",
    "                ).copy()\n",
    "            )\n",
    "        )\n",
    "        df_owner_train[\"label\"] = 1\n",
    "        df_owner_train[\"mask\"] = -2  # To use as sklearn cv mask\n",
    "\n",
    "        # Restrict amount of overall train samples. (For evaluation of Training Delay)\n",
    "        if limit_train_samples:\n",
    "            df_owner_train = df_owner_train.sample(\n",
    "                n=min(len(df_owner_train), limit_train_samples), random_state=seed\n",
    "            )\n",
    "\n",
    "        # Normalize owner training data and memorize scaler\n",
    "        if not scaler_global:\n",
    "            df_owner_train, scalers = utils_custom_scale(\n",
    "                df_owner_train,\n",
    "                scale_cols=\"X\",\n",
    "                feature_cols=feature_cols,\n",
    "                scaler_name=scaler,\n",
    "                scope=scaler_scope,\n",
    "                plot=False,\n",
    "            )\n",
    "\n",
    "        # PREPARE VALIDATION DATA\n",
    "        # ============================\n",
    "\n",
    "        df_valid = df_valid_original.copy()\n",
    "\n",
    "        # Prepate Validation Data Owner\n",
    "        # --------------------------------\n",
    "\n",
    "        # Select random owner samples\n",
    "        df_owner_valid = (\n",
    "            df_valid.query(\"subject == @owner\")\n",
    "            .groupby(\"session\", group_keys=False)\n",
    "            .apply(\n",
    "                lambda x: x.sample(n=min(len(x), samples_per_session_test), random_state=seed)\n",
    "            )\n",
    "            .copy()\n",
    "        )\n",
    "        df_owner_valid[\"label\"] = 1\n",
    "        df_owner_valid[\"mask\"] = -1\n",
    "\n",
    "        # Restrict amount of overall test samples. (For evaluation of Detection Delay)\n",
    "        if limit_test_samples:\n",
    "            df_owner_valid = df_owner_valid.sample(\n",
    "                n=min(len(df_owner_valid), limit_test_samples), random_state=seed\n",
    "            )\n",
    "\n",
    "        # Normalize with owner's scaler\n",
    "        if not scaler_global:\n",
    "            df_owner_valid = utils_scale_X(df_owner_valid, scalers[owner])\n",
    "        \n",
    "        # Select Impostors for attack scenarios\n",
    "        impostors = [sub for sub in subjects if sub != owner]\n",
    "\n",
    "        # Prepate Validation Data Impostors\n",
    "        # --------------------------------\n",
    "\n",
    "        # Select random samples from every impostors\n",
    "        df_impostors = pd.DataFrame()\n",
    "        for idx, impostor in enumerate(impostors):\n",
    "            # Select impostor validation set\n",
    "            df_impostor = (\n",
    "                df_valid.query(\"subject == @impostor\")\n",
    "                .groupby(\"session\", group_keys=False)\n",
    "                .apply(\n",
    "                    lambda x: x.sample(\n",
    "                        n=min(len(x), samples_per_session_test), random_state=seed\n",
    "                    ).copy()\n",
    "                )\n",
    "            )\n",
    "            df_impostor[\"label\"] = -1\n",
    "            df_impostor[\"mask\"] = idx\n",
    "            df_impostors = pd.concat([df_impostors, df_impostor], ignore_index=True)\n",
    "\n",
    "        # Normalize with owner's scaler\n",
    "        if not scaler_global:\n",
    "            df_impostors = utils_scale_X(df_impostors, scalers[owner])\n",
    "\n",
    "            \n",
    "        # FINALIZE TRAINING & VALIDATION DATA\n",
    "        # ====================================\n",
    "\n",
    "        # Concat training and validation data\n",
    "        df_cv_scenarios = pd.concat(\n",
    "            [df_owner_train, df_owner_valid, df_impostors], ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Predict deep features, if model provided\n",
    "        if deep_model:\n",
    "            df_cv_scenarios = utils_generate_deep_features(\n",
    "                df_cv_scenarios, deep_model, model_variant\n",
    "            )\n",
    "\n",
    "        # Shuffle samples\n",
    "        df_cv_scenarios = df_cv_scenarios.sample(frac=1, random_state=seed)\n",
    "\n",
    "        # Pass over data along with owner & impostor info\n",
    "        yield (df_cv_scenarios, owner, impostors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    # Need DataSetLoader here\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    module_path = os.path.abspath(os.path.join(\"..\"))  # supposed to be parent folder\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    from src.utility.dataset_loader_hdf5 import DatasetLoader\n",
    "\n",
    "    hmog = DatasetLoader(\n",
    "        hdf5_file=Path.cwd().parent / \"data\" / \"processed\" / \"hmog_dataset.hdf5\",\n",
    "        table_name=\"sensors_25hz\",\n",
    "        max_subjects=2,\n",
    "        task_types=[2],\n",
    "        exclude_subjects=[],\n",
    "        exclude_cols=[],\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # Dummy dataframe\n",
    "    df_temp = pd.DataFrame(\n",
    "        {\n",
    "            \"subject\": [sub for sub in \"a\" * 8 + \"b\" * 8 + \"c\" * 8],\n",
    "            \"session\": sorted([i + 1 for i in range(12)] * 2),\n",
    "            \"task_type\": [1, 1, 2, 2] * 6,\n",
    "            \"X\": [\n",
    "                *[[[0, 0, 0], [-1, 5, 10]]] * 8,\n",
    "                *[[[0, 0, 0], [-1, 0, 1]]] * 8,\n",
    "                *[[[0, 0, 0], [-75, 0, 75]]] * 8,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Initial dummy dataframe:\")\n",
    "    display(df_temp)\n",
    "\n",
    "    for df_temp_scenarios, temp_owner, temp_impostors in utils_generate_cv_scenarios(\n",
    "        df_temp,\n",
    "        max_train_samples=1,\n",
    "        max_test_samples=1,\n",
    "        seed=123,\n",
    "        max_scenarios=None,\n",
    "        limit_train_samples=None,\n",
    "        limit_test_samples=None,\n",
    "        scaler=\"minmax_fix\",\n",
    "        scaler_global=False,\n",
    "        scaler_scope=\"subject\",\n",
    "        deep_model=None,\n",
    "        dims=None,\n",
    "        feature_cols=[\"gyr_x\",\"gyr_y\", \"gyr_z\"],\n",
    "    ):\n",
    "        print(f\"\\nCV set with owner '{temp_owner}' and impostors '{temp_impostors}':\")\n",
    "        display(df_temp_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "   \n",
    "- Label \"1\" -> Owner\n",
    "- Label \"-1\" -> Impostor\n",
    "- Mask \"-2\" -> Training data (only from owner)\n",
    "- Mask \"-1\" -> Validation/Testing data (from owner)\n",
    "- Mask \"0\", \"1\", ... -> Individual impostors\n",
    "- For every iteration (which will serve for CV later), another subject is selected as \"owner\", all others become \"impostors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils_create_cv_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_create_cv_splits(cv_mask, seed):\n",
    "    \"\"\"\"Create cross-validation mask with train-valid pairs.\n",
    "    \n",
    "    See e.g. https://stackoverflow.com/a/37591377\n",
    "    \n",
    "    Arguments:\n",
    "        cv_mask {np.ndarray} --\n",
    "        \n",
    "    Return:\n",
    "        {list} -- List of tuple: (<train indices>, <valid indices>)\n",
    "        \n",
    "    \"\"\"\n",
    "    train_idx_owner = np.where(cv_mask == -2)[0]\n",
    "    valid_idx_owner = np.where(cv_mask == -1)[0]\n",
    "\n",
    "    cv_splits = []\n",
    "    for idx in range(cv_mask.max() + 1):\n",
    "        # Impostor validation indices\n",
    "        valid_idx_impostor = np.where(cv_mask == idx)[0]\n",
    "\n",
    "        # Balance classes\n",
    "        min_samples = min(valid_idx_owner.shape[0], valid_idx_impostor.shape[0])\n",
    "        np.random.seed(seed + idx)\n",
    "        valid_idx_owner_samp = np.random.choice(\n",
    "            valid_idx_owner, size=min_samples, replace=False\n",
    "        )\n",
    "        np.random.seed(seed + idx)\n",
    "        valid_idx_impostor_samp = np.random.choice(\n",
    "            valid_idx_impostor, size=min_samples, replace=False\n",
    "        )\n",
    "\n",
    "        # Concat owner & impostor validation indices\n",
    "        valid_idx_both = np.hstack([valid_idx_owner_samp, valid_idx_impostor_samp])\n",
    "\n",
    "        # Add train/valid pair to cv\n",
    "        cv_splits.append((list(train_idx_owner), list(valid_idx_both)))\n",
    "\n",
    "    return cv_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    # Mask Explained:\n",
    "    # -2 => Training data (owner)\n",
    "    # -1 => Validation data (owner)\n",
    "    # 0+ => Validation impostors\n",
    "    #              Indices:    0   1   2   3   4   5  6  7  8  9  10 11 12 13 14 15\n",
    "    dummy_cv_mask = np.array([-2, -2, -1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 2, 2, 2, -2])\n",
    "\n",
    "    # Generate tuples of training data and validation data, one tuple for each impostor (0, 1, 2).\n",
    "    # Training data (1st list in tuple) contains only indices of owner training data (-2)\n",
    "    # Validation data (2nd list in tuple) contains  indices of validation data from owner (-1) and\n",
    "    # from a single impostor (0+), each 50 %\n",
    "    splits = utils_create_cv_splits(dummy_cv_mask, seed=123)\n",
    "    [print(s) for s in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contauth",
   "language": "python",
   "name": "contauth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
